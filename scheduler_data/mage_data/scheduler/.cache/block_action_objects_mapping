{"block_file": {"data_exporters/export_qb_customers.py:data_exporter:python:export qb customers": {"content": "from mage_ai.settings.repo import get_repo_path\nfrom mage_ai.io.config import ConfigFileLoader\nfrom mage_ai.io.postgres import Postgres\nfrom pandas import DataFrame\nfrom os import path\nimport pandas as pd\n\nif 'data_exporter' not in globals():\n    from mage_ai.data_preparation.decorators import data_exporter\n\n\n@data_exporter\ndef export_data_to_postgres(df: DataFrame, **kwargs) -> None:\n    \"\"\"\n    Exporta datos a PostgreSQL usando UPSERT fila por fila para garantizar idempotencia.\n    Re-ejecutar con los mismos datos no duplicar\u00e1 filas.\n\n    Docs: https://docs.mage.ai/design/data-loading#postgresql\n    \"\"\"\n    if df.empty:\n        print(\"DataFrame vac\u00edo, no hay datos para exportar\")\n        return\n    \n    schema_name = 'raw'\n    table_name = 'qb_customer'\n    config_path = path.join(get_repo_path(), 'io_config.yaml')\n    config_profile = 'default'\n    \n    print(f\"Exportando {len(df)} registros a {schema_name}.{table_name}\")\n    print(\"M\u00e9todo: UPSERT fila por fila\")\n    \n    with Postgres.with_config(ConfigFileLoader(config_path, config_profile)) as loader:\n        # Limpiar cualquier transacci\u00f3n pendiente al inicio\n        try:\n            loader.execute(\"ROLLBACK;\")\n            print(\"Limpieza inicial de transacciones completada\")\n        except:\n            pass  # Ignorar si no hay transacci\u00f3n pendiente\n        \n        # Verificar conexi\u00f3n\n        try:\n            test_result = loader.execute(\"SELECT version();\")\n            print(f\"Conectado a PostgreSQL: {test_result[0][0] if test_result else 'Versi\u00f3n no disponible'}\")\n        except Exception as e:\n            print(f\"Error verificando conexi\u00f3n: {e}\")\n            return\n        \n        # Asumir que esquema 'raw' existe\n        print(f\"Usando esquema '{schema_name}' (asumiendo que existe)\")\n        \n        # Verificar si la tabla existe\n        verify_table_sql = f\"\"\"\n        SELECT table_name \n        FROM information_schema.tables \n        WHERE table_schema = '{schema_name}' AND table_name = '{table_name}';\n        \"\"\"\n        \n        try:\n            table_exists = loader.execute(verify_table_sql)\n            if table_exists and len(table_exists) > 0:\n                print(f\"\u2713 Tabla '{schema_name}.{table_name}' ya existe\")\n            else:\n                print(f\"Tabla '{schema_name}.{table_name}' no existe, cre\u00e1ndola...\")\n                \n                # Crear tabla si no existe\n                create_table_sql = f\"\"\"\n                CREATE TABLE {schema_name}.{table_name} (\n                    id VARCHAR(50) PRIMARY KEY,\n                    payload JSONB,\n                    ingested_at_utc TIMESTAMPTZ,\n                    extract_window_start_utc TIMESTAMPTZ,\n                    extract_window_end_utc TIMESTAMPTZ,\n                    page_number INTEGER,\n                    page_size INTEGER,\n                    request_payload JSONB\n                );\n                \"\"\"\n                \n                result = loader.execute(create_table_sql)\n                print(f\"Comando CREATE TABLE ejecutado - Resultado: {result}\")\n                \n                # Verificar nuevamente que se cre\u00f3\n                table_exists_after = loader.execute(verify_table_sql)\n                if table_exists_after and len(table_exists_after) > 0:\n                    print(f\"Tabla '{schema_name}.{table_name}' creada exitosamente\")\n                else:\n                    print(f\"Tabla '{schema_name}.{table_name}' podr\u00eda no haberse creado, pero continuando...\")\n                    \n        except Exception as e:\n            print(f\"Error verificando/creando tabla '{schema_name}.{table_name}': {e}\")\n            print(\"Continuando con el procesamiento...\")\n        \n        # Contadores para estad\u00edsticas\n        inserted_count = 0\n        updated_count = 0\n        error_count = 0\n        \n        print(f\"Procesando {len(df)} registros individualmente...\")\n        \n        # Procesar cada fila del DataFrame\n        for index_num, (index, row) in enumerate(df.iterrows()):\n            try:\n                # Preparar valores para la consulta, escapando comillas simples\n                values_list = []\n                update_clauses = []\n                \n                for col in df.columns:\n                    value = row[col]\n                    # Convertir valor a string SQL apropiado\n                    if value is None or pd.isna(value):\n                        sql_value = 'NULL'\n                    elif isinstance(value, str):\n                        # Escapar comillas simples y envolver en comillas\n                        escaped_value = value.replace(\"'\", \"''\")\n                        sql_value = f\"'{escaped_value}'\"\n                    else:\n                        # Para otros tipos (n\u00fameros, etc.), convertir a string\n                        sql_value = f\"'{str(value)}'\"\n                    \n                    values_list.append(sql_value)\n                    \n                    if col != 'id':  # No actualizar la clave primaria\n                        update_clauses.append(f\"{col} = EXCLUDED.{col}\")\n                \n                # Construir consulta UPSERT con valores directos\n                columns_str = ', '.join(df.columns)\n                values_str = ', '.join(values_list)\n                update_str = ', '.join(update_clauses)\n                \n                # UPSERT sin transacci\u00f3n expl\u00edcita (Mage AI maneja las transacciones)\n                upsert_sql = f\"\"\"\n                INSERT INTO {schema_name}.{table_name} ({columns_str}) \n                VALUES ({values_str})\n                ON CONFLICT (id) \n                DO UPDATE SET {update_str};\n                \"\"\"\n                \n                # Ejecutar UPSERT con manejo de transacci\u00f3n\n                try:\n                    loader.execute(upsert_sql)\n                    inserted_count += 1\n                except Exception as upsert_error:\n                    # Si el UPSERT falla, hacer rollback inmediato\n                    try:\n                        loader.execute(\"ROLLBACK;\")\n                    except:\n                        pass\n                    raise upsert_error  # Re-lanzar para el manejo principal\n                \n                # Mostrar progreso cada 100 registros\n                processed_count = index_num + 1\n                if processed_count % 100 == 0:\n                    print(f\"Procesados: {processed_count}/{len(df)} registros\")\n                \n            except Exception as e:\n                error_count += 1\n                record_id = row.get('id', 'N/A')\n                record_num = index_num + 1\n                print(f\"Error procesando registro {record_num} (ID: {record_id}): {e}\")\n                \n                # Intentar hacer rollback si hay transacci\u00f3n colgada\n                try:\n                    loader.execute(\"ROLLBACK;\")\n                except:\n                    pass\n                \n                continue\n        \n        # Estad\u00edsticas finales\n        try:\n            final_count_sql = f\"SELECT COUNT(*) FROM {schema_name}.{table_name};\"\n            final_result = loader.execute(final_count_sql)\n            final_count = final_result[0][0] if final_result else 0\n        except Exception as e:\n            print(f\"Error obteniendo estad\u00edsticas finales: {e}\")\n            # Intentar rollback y luego obtener estad\u00edsticas\n            try:\n                loader.execute(\"ROLLBACK;\")\n                final_result = loader.execute(final_count_sql)\n                final_count = final_result[0][0] if final_result else 0\n            except:\n                final_count = \"N/A\"\n        \n        print(f\"\\nUPSERT FILA POR FILA COMPLETADO\")\n        print(f\"Total registros procesados: {len(df)}\")\n        print(f\"Registros insertados (nuevos): {inserted_count}\")\n        print(f\"Registros actualizados (existentes): {updated_count}\")\n        print(f\"Errores: {error_count}\")\n        print(f\"Total registros en tabla: {final_count}\")\n        print(\"Idempotencia garantizada: re-ejecutar con los mismos IDs no duplicar\u00e1 filas\")\n", "file_path": "data_exporters/export_qb_customers.py", "language": "python", "type": "data_exporter", "uuid": "export_qb_customers"}, "data_exporters/export_qb_invoices.py:data_exporter:python:export qb invoices": {"content": "from mage_ai.settings.repo import get_repo_path\nfrom mage_ai.io.config import ConfigFileLoader\nfrom mage_ai.io.postgres import Postgres\nfrom pandas import DataFrame\nfrom os import path\nimport pandas as pd\n\nif 'data_exporter' not in globals():\n    from mage_ai.data_preparation.decorators import data_exporter\n\n\n@data_exporter\ndef export_data_to_postgres(df: DataFrame, **kwargs) -> None:\n    \"\"\"\n    Exporta datos a PostgreSQL usando UPSERT fila por fila para garantizar idempotencia.\n    Re-ejecutar con los mismos datos no duplicar\u00e1 filas.\n\n    Docs: https://docs.mage.ai/design/data-loading#postgresql\n    \"\"\"\n    if df.empty:\n        print(\"DataFrame vac\u00edo, no hay datos para exportar\")\n        return\n    \n    schema_name = 'raw'\n    table_name = 'qb_invoices'\n    config_path = path.join(get_repo_path(), 'io_config.yaml')\n    config_profile = 'default'\n    \n    print(f\"Exportando {len(df)} registros a {schema_name}.{table_name}\")\n    print(\"M\u00e9todo: UPSERT fila por fila\")\n    \n    with Postgres.with_config(ConfigFileLoader(config_path, config_profile)) as loader:\n        # Limpiar cualquier transacci\u00f3n pendiente al inicio\n        try:\n            loader.execute(\"ROLLBACK;\")\n            print(\"Limpieza inicial de transacciones completada\")\n        except:\n            pass  # Ignorar si no hay transacci\u00f3n pendiente\n        \n        # Verificar conexi\u00f3n\n        try:\n            test_result = loader.execute(\"SELECT version();\")\n            print(f\"Conectado a PostgreSQL: {test_result[0][0] if test_result else 'Versi\u00f3n no disponible'}\")\n        except Exception as e:\n            print(f\"Error verificando conexi\u00f3n: {e}\")\n            return\n        \n        # Asumir que esquema 'raw' existe\n        print(f\"Usando esquema '{schema_name}' (asumiendo que existe)\")\n        \n        # Verificar si la tabla existe\n        verify_table_sql = f\"\"\"\n        SELECT table_name \n        FROM information_schema.tables \n        WHERE table_schema = '{schema_name}' AND table_name = '{table_name}';\n        \"\"\"\n        \n        try:\n            table_exists = loader.execute(verify_table_sql)\n            if table_exists and len(table_exists) > 0:\n                print(f\"\u2713 Tabla '{schema_name}.{table_name}' ya existe\")\n            else:\n                print(f\"Tabla '{schema_name}.{table_name}' no existe, cre\u00e1ndola...\")\n                \n                # Crear tabla si no existe\n                create_table_sql = f\"\"\"\n                CREATE TABLE {schema_name}.{table_name} (\n                    id VARCHAR(50) PRIMARY KEY,\n                    payload JSONB,\n                    ingested_at_utc TIMESTAMPTZ,\n                    extract_window_start_utc TIMESTAMPTZ,\n                    extract_window_end_utc TIMESTAMPTZ,\n                    page_number INTEGER,\n                    page_size INTEGER,\n                    request_payload JSONB\n                );\n                \"\"\"\n                \n                result = loader.execute(create_table_sql)\n                print(f\"Comando CREATE TABLE ejecutado - Resultado: {result}\")\n                \n                # Verificar nuevamente que se cre\u00f3\n                table_exists_after = loader.execute(verify_table_sql)\n                if table_exists_after and len(table_exists_after) > 0:\n                    print(f\"Tabla '{schema_name}.{table_name}' creada exitosamente\")\n                else:\n                    print(f\"Tabla '{schema_name}.{table_name}' podr\u00eda no haberse creado, pero continuando...\")\n                    \n        except Exception as e:\n            print(f\"Error verificando/creando tabla '{schema_name}.{table_name}': {e}\")\n            print(\"Continuando con el procesamiento...\")\n        \n        # Contadores para estad\u00edsticas\n        inserted_count = 0\n        updated_count = 0\n        error_count = 0\n        \n        print(f\"Procesando {len(df)} registros individualmente...\")\n        \n        # Procesar cada fila del DataFrame\n        for index_num, (index, row) in enumerate(df.iterrows()):\n            try:\n                # Preparar valores para la consulta, escapando comillas simples\n                values_list = []\n                update_clauses = []\n                \n                for col in df.columns:\n                    value = row[col]\n                    # Convertir valor a string SQL apropiado\n                    if value is None or pd.isna(value):\n                        sql_value = 'NULL'\n                    elif isinstance(value, str):\n                        # Escapar comillas simples y envolver en comillas\n                        escaped_value = value.replace(\"'\", \"''\")\n                        sql_value = f\"'{escaped_value}'\"\n                    else:\n                        # Para otros tipos (n\u00fameros, etc.), convertir a string\n                        sql_value = f\"'{str(value)}'\"\n                    \n                    values_list.append(sql_value)\n                    \n                    if col != 'id':  # No actualizar la clave primaria\n                        update_clauses.append(f\"{col} = EXCLUDED.{col}\")\n                \n                # Construir consulta UPSERT con valores directos\n                columns_str = ', '.join(df.columns)\n                values_str = ', '.join(values_list)\n                update_str = ', '.join(update_clauses)\n                \n                # UPSERT sin transacci\u00f3n expl\u00edcita (Mage AI maneja las transacciones)\n                upsert_sql = f\"\"\"\n                INSERT INTO {schema_name}.{table_name} ({columns_str}) \n                VALUES ({values_str})\n                ON CONFLICT (id) \n                DO UPDATE SET {update_str};\n                \"\"\"\n                \n                # Ejecutar UPSERT con manejo de transacci\u00f3n\n                try:\n                    loader.execute(upsert_sql)\n                    inserted_count += 1\n                except Exception as upsert_error:\n                    # Si el UPSERT falla, hacer rollback inmediato\n                    try:\n                        loader.execute(\"ROLLBACK;\")\n                    except:\n                        pass\n                    raise upsert_error  # Re-lanzar para el manejo principal\n                \n                # Mostrar progreso cada 100 registros\n                processed_count = index_num + 1\n                if processed_count % 100 == 0:\n                    print(f\"Procesados: {processed_count}/{len(df)} registros\")\n                \n            except Exception as e:\n                error_count += 1\n                record_id = row.get('id', 'N/A')\n                record_num = index_num + 1\n                print(f\"Error procesando registro {record_num} (ID: {record_id}): {e}\")\n                \n                # Intentar hacer rollback si hay transacci\u00f3n colgada\n                try:\n                    loader.execute(\"ROLLBACK;\")\n                except:\n                    pass\n                \n                continue\n        \n        # Estad\u00edsticas finales\n        try:\n            final_count_sql = f\"SELECT COUNT(*) FROM {schema_name}.{table_name};\"\n            final_result = loader.execute(final_count_sql)\n            final_count = final_result[0][0] if final_result else 0\n        except Exception as e:\n            print(f\"Error obteniendo estad\u00edsticas finales: {e}\")\n            # Intentar rollback y luego obtener estad\u00edsticas\n            try:\n                loader.execute(\"ROLLBACK;\")\n                final_result = loader.execute(final_count_sql)\n                final_count = final_result[0][0] if final_result else 0\n            except:\n                final_count = \"N/A\"\n        \n        print(f\"\\nUPSERT FILA POR FILA COMPLETADO\")\n        print(f\"Total registros procesados: {len(df)}\")\n        print(f\"Registros insertados (nuevos): {inserted_count}\")\n        print(f\"Registros actualizados (existentes): {updated_count}\")\n        print(f\"Errores: {error_count}\")\n        print(f\"Total registros en tabla: {final_count}\")\n        print(\"Idempotencia garantizada: re-ejecutar con los mismos IDs no duplicar\u00e1 filas\")\n", "file_path": "data_exporters/export_qb_invoices.py", "language": "python", "type": "data_exporter", "uuid": "export_qb_invoices"}, "data_exporters/export_qb_items.py:data_exporter:python:export qb items": {"content": "from mage_ai.settings.repo import get_repo_path\nfrom mage_ai.io.config import ConfigFileLoader\nfrom mage_ai.io.postgres import Postgres\nfrom pandas import DataFrame\nfrom os import path\nimport pandas as pd\n\nif 'data_exporter' not in globals():\n    from mage_ai.data_preparation.decorators import data_exporter\n\n\n@data_exporter\ndef export_data_to_postgres(df: DataFrame, **kwargs) -> None:\n    \"\"\"\n    Exporta datos a PostgreSQL usando UPSERT fila por fila para garantizar idempotencia.\n    Re-ejecutar con los mismos datos no duplicar\u00e1 filas.\n\n    Docs: https://docs.mage.ai/design/data-loading#postgresql\n    \"\"\"\n    if df.empty:\n        print(\"DataFrame vac\u00edo, no hay datos para exportar\")\n        return\n    \n    schema_name = 'raw'\n    table_name = 'qb_item'\n    config_path = path.join(get_repo_path(), 'io_config.yaml')\n    config_profile = 'default'\n    \n    print(f\"Exportando {len(df)} registros a {schema_name}.{table_name}\")\n    print(\"M\u00e9todo: UPSERT fila por fila\")\n    \n    with Postgres.with_config(ConfigFileLoader(config_path, config_profile)) as loader:\n        # Limpiar cualquier transacci\u00f3n pendiente al inicio\n        try:\n            loader.execute(\"ROLLBACK;\")\n            print(\"Limpieza inicial de transacciones completada\")\n        except:\n            pass  # Ignorar si no hay transacci\u00f3n pendiente\n        \n        # Verificar conexi\u00f3n\n        try:\n            test_result = loader.execute(\"SELECT version();\")\n            print(f\"Conectado a PostgreSQL: {test_result[0][0] if test_result else 'Versi\u00f3n no disponible'}\")\n        except Exception as e:\n            print(f\"Error verificando conexi\u00f3n: {e}\")\n            return\n        \n        # Asumir que esquema 'raw' existe\n        print(f\"Usando esquema '{schema_name}' (asumiendo que existe)\")\n        \n        # Verificar si la tabla existe\n        verify_table_sql = f\"\"\"\n        SELECT table_name \n        FROM information_schema.tables \n        WHERE table_schema = '{schema_name}' AND table_name = '{table_name}';\n        \"\"\"\n        \n        try:\n            table_exists = loader.execute(verify_table_sql)\n            if table_exists and len(table_exists) > 0:\n                print(f\"\u2713 Tabla '{schema_name}.{table_name}' ya existe\")\n            else:\n                print(f\"Tabla '{schema_name}.{table_name}' no existe, cre\u00e1ndola...\")\n                \n                # Crear tabla si no existe\n                create_table_sql = f\"\"\"\n                CREATE TABLE {schema_name}.{table_name} (\n                    id VARCHAR(50) PRIMARY KEY,\n                    payload JSONB,\n                    ingested_at_utc TIMESTAMPTZ,\n                    extract_window_start_utc TIMESTAMPTZ,\n                    extract_window_end_utc TIMESTAMPTZ,\n                    page_number INTEGER,\n                    page_size INTEGER,\n                    request_payload JSONB\n                );\n                \"\"\"\n                \n                result = loader.execute(create_table_sql)\n                print(f\"Comando CREATE TABLE ejecutado - Resultado: {result}\")\n                \n                # Verificar nuevamente que se cre\u00f3\n                table_exists_after = loader.execute(verify_table_sql)\n                if table_exists_after and len(table_exists_after) > 0:\n                    print(f\"Tabla '{schema_name}.{table_name}' creada exitosamente\")\n                else:\n                    print(f\"Tabla '{schema_name}.{table_name}' podr\u00eda no haberse creado, pero continuando...\")\n                    \n        except Exception as e:\n            print(f\"Error verificando/creando tabla '{schema_name}.{table_name}': {e}\")\n            print(\"Continuando con el procesamiento...\")\n        \n        # Contadores para estad\u00edsticas\n        inserted_count = 0\n        updated_count = 0\n        error_count = 0\n        \n        print(f\"Procesando {len(df)} registros individualmente...\")\n        \n        # Procesar cada fila del DataFrame\n        for index_num, (index, row) in enumerate(df.iterrows()):\n            try:\n                # Preparar valores para la consulta, escapando comillas simples\n                values_list = []\n                update_clauses = []\n                \n                for col in df.columns:\n                    value = row[col]\n                    # Convertir valor a string SQL apropiado\n                    if value is None or pd.isna(value):\n                        sql_value = 'NULL'\n                    elif isinstance(value, str):\n                        # Escapar comillas simples y envolver en comillas\n                        escaped_value = value.replace(\"'\", \"''\")\n                        sql_value = f\"'{escaped_value}'\"\n                    else:\n                        # Para otros tipos (n\u00fameros, etc.), convertir a string\n                        sql_value = f\"'{str(value)}'\"\n                    \n                    values_list.append(sql_value)\n                    \n                    if col != 'id':  # No actualizar la clave primaria\n                        update_clauses.append(f\"{col} = EXCLUDED.{col}\")\n                \n                # Construir consulta UPSERT con valores directos\n                columns_str = ', '.join(df.columns)\n                values_str = ', '.join(values_list)\n                update_str = ', '.join(update_clauses)\n                \n                # UPSERT sin transacci\u00f3n expl\u00edcita (Mage AI maneja las transacciones)\n                upsert_sql = f\"\"\"\n                INSERT INTO {schema_name}.{table_name} ({columns_str}) \n                VALUES ({values_str})\n                ON CONFLICT (id) \n                DO UPDATE SET {update_str};\n                \"\"\"\n                \n                # Ejecutar UPSERT con manejo de transacci\u00f3n\n                try:\n                    loader.execute(upsert_sql)\n                    inserted_count += 1\n                except Exception as upsert_error:\n                    # Si el UPSERT falla, hacer rollback inmediato\n                    try:\n                        loader.execute(\"ROLLBACK;\")\n                    except:\n                        pass\n                    raise upsert_error  # Re-lanzar para el manejo principal\n                \n                # Mostrar progreso cada 100 registros\n                processed_count = index_num + 1\n                if processed_count % 100 == 0:\n                    print(f\"Procesados: {processed_count}/{len(df)} registros\")\n                \n            except Exception as e:\n                error_count += 1\n                record_id = row.get('id', 'N/A')\n                record_num = index_num + 1\n                print(f\"Error procesando registro {record_num} (ID: {record_id}): {e}\")\n                \n                # Intentar hacer rollback si hay transacci\u00f3n colgada\n                try:\n                    loader.execute(\"ROLLBACK;\")\n                except:\n                    pass\n                \n                continue\n        \n        # Estad\u00edsticas finales\n        try:\n            final_count_sql = f\"SELECT COUNT(*) FROM {schema_name}.{table_name};\"\n            final_result = loader.execute(final_count_sql)\n            final_count = final_result[0][0] if final_result else 0\n        except Exception as e:\n            print(f\"Error obteniendo estad\u00edsticas finales: {e}\")\n            # Intentar rollback y luego obtener estad\u00edsticas\n            try:\n                loader.execute(\"ROLLBACK;\")\n                final_result = loader.execute(final_count_sql)\n                final_count = final_result[0][0] if final_result else 0\n            except:\n                final_count = \"N/A\"\n        \n        print(f\"\\nUPSERT FILA POR FILA COMPLETADO\")\n        print(f\"Total registros procesados: {len(df)}\")\n        print(f\"Registros insertados (nuevos): {inserted_count}\")\n        print(f\"Registros actualizados (existentes): {updated_count}\")\n        print(f\"Errores: {error_count}\")\n        print(f\"Total registros en tabla: {final_count}\")\n        print(\"Idempotencia garantizada: re-ejecutar con los mismos IDs no duplicar\u00e1 filas\")\n", "file_path": "data_exporters/export_qb_items.py", "language": "python", "type": "data_exporter", "uuid": "export_qb_items"}, "data_exporters/export_titanic_clean.py:data_exporter:python:export titanic clean": {"content": "from mage_ai.io.file import FileIO\nfrom pandas import DataFrame\n\nif 'data_exporter' not in globals():\n    from mage_ai.data_preparation.decorators import data_exporter\n\n\n@data_exporter\ndef export_data_to_file(df: DataFrame, **kwargs) -> None:\n    \"\"\"\n    Template for exporting data to filesystem.\n\n    Docs: https://docs.mage.ai/design/data-loading#example-loading-data-from-a-file\n    \"\"\"\n    filepath = 'titanic_clean.csv'\n    FileIO().export(df, filepath)\n", "file_path": "data_exporters/export_titanic_clean.py", "language": "python", "type": "data_exporter", "uuid": "export_titanic_clean"}, "data_exporters/ny_taxi_exporter.py:data_exporter:python:ny taxi exporter": {"content": "from mage_ai.settings.repo import get_repo_path\nfrom mage_ai.io.config import ConfigFileLoader\nfrom mage_ai.io.postgres import Postgres\nfrom pandas import DataFrame\nfrom os import path\n\nif 'data_exporter' not in globals():\n    from mage_ai.data_preparation.decorators import data_exporter\n\n\n@data_exporter\ndef export_data_to_postgres(df: DataFrame, **kwargs) -> None:\n    \"\"\"\n    Template for exporting data to a PostgreSQL database.\n    Specify your configuration settings in 'io_config.yaml'.\n\n    Docs: https://docs.mage.ai/design/data-loading#postgresql\n    \"\"\"\n    schema_name = 'public'  # Specify the name of the schema to export data to\n    table_name = 'taxi_trips'  # Specify the name of the table to export data to\n    config_path = path.join(get_repo_path(), 'io_config.yaml')\n    config_profile = 'default'\n\n    with Postgres.with_config(ConfigFileLoader(config_path, config_profile)) as loader:\n        loader.export(\n            df,\n            schema_name,\n            table_name,\n            index=False,  # Specifies whether to include index in exported table\n            if_exists='replace',  # Specify resolution policy if table name already exists\n        )\n        ", "file_path": "data_exporters/ny_taxi_exporter.py", "language": "python", "type": "data_exporter", "uuid": "ny_taxi_exporter"}, "data_loaders/ingest.py:data_loader:python:ingest": {"content": "import io\nimport pandas as pd\nimport requests\nif 'data_loader' not in globals():\n    from mage_ai.data_preparation.decorators import data_loader\nif 'test' not in globals():\n    from mage_ai.data_preparation.decorators import test\n\n\n@data_loader\ndef load_data_from_api(*args, **kwargs):\n    \"\"\"\n    Template for loading data from API\n    \"\"\"\n    url = ''\n    response = requests.get(url)\n\n    return pd.read_csv(io.StringIO(response.text), sep=',')\n\n\n@test\ndef test_output(output, *args) -> None:\n    \"\"\"\n    Template code for testing the output of the block.\n    \"\"\"\n    assert output is not None, 'The output is undefined'\n", "file_path": "data_loaders/ingest.py", "language": "python", "type": "data_loader", "uuid": "ingest"}, "data_loaders/ingest_qb_customers.py:data_loader:python:ingest qb customers": {"content": "import requests\nimport json\nimport pandas as pd\nfrom datetime import datetime, timedelta\nfrom mage_ai.data_preparation.shared.secrets import get_secret_value\nimport time\n\n\nif 'data_loader' not in globals():\n    from mage_ai.data_preparation.decorators import data_loader\nif 'test' not in globals():\n    from mage_ai.data_preparation.decorators import test\n\ndef _refrescar_access_token():\n\n    refresh_token = get_secret_value('qb_refresh_token')\n    client_id = get_secret_value('qb_client_id')\n    client_secret = get_secret_value('qb_client_secret')\n    \n    url_base = 'https://oauth.platform.intuit.com/oauth2/v1/tokens/bearer'\n\n    headers = {\n        'Content-Type': 'application/x-www-form-urlencoded',\n        'Accept': 'application/json'\n    }\n    \n    data = {\n        'grant_type': 'refresh_token',\n        'refresh_token': refresh_token,\n        'client_id': client_id,\n        'client_secret': client_secret\n    }\n    \n    try:\n        print('Post para refrescar token')\n        response = requests.post(url_base, headers=headers, data=data, timeout=60)\n        response.raise_for_status()\n        token_data = response.json()\n        new_access_token = token_data.get('access_token')\n        new_refresh_token = token_data.get('refresh_token')\n\n        if new_access_token:\n            print('Exito al refrescar token')\n            return new_access_token, new_refresh_token\n        else:\n            raise ValueError(\"Error al solicitar nuevo token\")\n            \n    except requests.exceptions.RequestException as e:\n        print(f'Error al refrescar token: {e}')\n        return None, None\n    except json.JSONDecodeError as e:\n        print(f'Error al decodificar respuesta JSON: {e}')\n        return None, None\n\ndef _fetch_qb_data(realm_id, access_token, query, base_url, minor_version, start_position=1, max_results=1000):\n\n    if not base_url or not minor_version:\n        raise ValueError(\"Se requiere una URL base y el minor version\")    \n\n    headers = {\n        'Authorization': f'Bearer {access_token}',\n        'Accept': 'application/json',\n        'Content-Type': 'text/plain'\n    }\n\n    # paginaci\u00f3n \n    paginated_query = f\"{query} STARTPOSITION {start_position} MAXRESULTS {max_results}\"\n\n    params = {\n        'query': paginated_query,\n        'minorversion': minor_version\n    }\n\n    url = f\"{base_url.rstrip('/')}/v3/company/{realm_id}/query\"\n    \n    #  reintentos con backoff exponencial\n    max_retries = 5\n    base_timeout = 60\n    base_delay = 1  # delay inicial para backoff exponencial\n    \n    # circuit breaker\n    consecutive_failures = getattr(_fetch_qb_data, '_consecutive_failures', 0)\n    circuit_open = getattr(_fetch_qb_data, '_circuit_open_until', 0)\n    \n    # Verificar si circuit breaker est\u00e1 abierto\n    current_time = time.time()\n    if current_time < circuit_open:\n        remaining_time = int(circuit_open - current_time)\n        print(f'CIRCUIT BREAKER ABIERTO - Esperando {remaining_time}s m\u00e1s antes de reintentar')\n        time.sleep(min(remaining_time, 30))  # Esperar m\u00e1ximo 30s en esta llamada\n        return None\n    \n    for attempt in range(max_retries):\n        # timeout incremental\n        current_timeout = base_timeout + (attempt * 30)\n        # backoff exponential delay (solo despu\u00e9s del primer intento)\n        if attempt > 0:\n            delay = base_delay * (2 ** (attempt - 1))  # 1s, 2s, 4s, 8s, 16s\n            print(f'Backoff exponencial: esperando {delay}s antes del intento {attempt + 1}')\n            time.sleep(delay)\n        \n        try:\n            print(f'Intento {attempt + 1}/{max_retries} - Request al API')\n            print(f'URL: {base_url}')\n            print(f'Query: {paginated_query}')\n            print(f'Posici\u00f3n: {start_position}, M\u00e1ximo: {max_results}')\n            print(f'Timeout: {current_timeout}s')\n            print(f'Fallos consecutivos: {consecutive_failures}')\n            \n            response = requests.get(url, headers=headers, params=params, timeout=current_timeout)\n            \n            # manejo de rate limits\n            if response.status_code == 429:  # Too Many Requests\n                retry_after = int(response.headers.get('Retry-After', 60))\n                print(f'RATE LIMIT EXCEDIDO - Esperando {retry_after}s (HTTP 429)')\n                time.sleep(retry_after)\n                continue  # Reintentar sin contar como fallo\n            \n            if response.status_code == 401:\n                print('Token expirado, refrescando...')\n                new_access_token, new_refresh_token = _refrescar_access_token()\n                \n                if new_access_token:\n                    headers['Authorization'] = f'Bearer {new_access_token}'\n                    print('Token refrescado, reintentando...')\n                    response = requests.get(url, headers=headers, params=params, timeout=current_timeout)\n                else:\n                    raise ValueError(\"Error cr\u00edtico: No se pudo refrescar el token\")\n            \n            response.raise_for_status()\n            data = response.json()\n            \n            # \u00c9XITO - Resetear circuit breaker\n            _fetch_qb_data._consecutive_failures = 0\n            _fetch_qb_data._circuit_open_until = 0\n            \n            print(f'Datos recibidos exitosamente en intento {attempt + 1}')\n            print(f'P\u00e1gina desde posici\u00f3n {start_position} obtenida correctamente')\n            return data\n            \n        except requests.exceptions.Timeout as e:\n            consecutive_failures += 1\n            print(f'TIMEOUT en intento {attempt + 1} despu\u00e9s de {current_timeout}s: {e}')\n            _handle_failure(attempt, max_retries, consecutive_failures, 'TIMEOUT')\n                \n        except requests.exceptions.ConnectionError as e:\n            consecutive_failures += 1\n            print(f'ERROR DE CONEXI\u00d3N en intento {attempt + 1}: {e}')\n            _handle_failure(attempt, max_retries, consecutive_failures, 'CONNECTION_ERROR')\n                \n        except requests.exceptions.RequestException as e:\n            consecutive_failures += 1\n            print(f'ERROR DE REQUEST en intento {attempt + 1}: {e}')\n            _handle_failure(attempt, max_retries, consecutive_failures, 'REQUEST_ERROR')\n                \n        except json.JSONDecodeError as e:\n            consecutive_failures += 1\n            print(f'ERROR DE JSON en intento {attempt + 1}: {e}')\n            _handle_failure(attempt, max_retries, consecutive_failures, 'JSON_ERROR')\n                \n        except Exception as e:\n            consecutive_failures += 1\n            print(f'ERROR INESPERADO en intento {attempt + 1}: {e}')\n            _handle_failure(attempt, max_retries, consecutive_failures, 'UNEXPECTED_ERROR')\n    \n    # todos los reintentos fallaron\n    _fetch_qb_data._consecutive_failures = consecutive_failures\n    _activate_circuit_breaker(consecutive_failures)\n    print(f'FALLO TOTAL: {max_retries} intentos agotados. Fallos consecutivos: {consecutive_failures}')\n    return None\n\ndef _handle_failure(attempt, max_retries, consecutive_failures, error_type):\n    if attempt == max_retries - 1:  # \u00daltimo intento\n        print(f'\u00daLTIMO INTENTO FALLADO - Tipo: {error_type}')\n        print(f'Fallos consecutivos acumulados: {consecutive_failures}')\n    else:\n        next_delay = 1 * (2 ** attempt)  # Pr\u00f3ximo delay exponencial\n        print(f'Preparando reintento con backoff exponencial de {next_delay}s')\n\ndef _activate_circuit_breaker(consecutive_failures):\n    if consecutive_failures >= 10:\n        circuit_open_time = 300\n        print('CIRCUIT BREAKER CR\u00cdTICO: 10+ fallos consecutivos - Pausando 5 minutos')\n    elif consecutive_failures >= 5:\n        circuit_open_time = 60\n        print('CIRCUIT BREAKER ACTIVADO: 5+ fallos consecutivos - Pausando 1 minuto')\n    else:\n        return  \n    \n    _fetch_qb_data._circuit_open_until = time.time() + circuit_open_time\n    _fetch_qb_data._consecutive_failures = consecutive_failures\n\n@data_loader\ndef load_data(*args, **kwargs):\n    \"\"\"\n    Template code for loading data from any source.\n    \n    Args:\n        fecha_inicio (str): Fecha de inicio en formato YYYY-MM-DD (requerido para backfill)\n        fecha_fin (str): Fecha de fin en formato YYYY-MM-DD (requerido para backfill)\n        chunk_days (int): N\u00famero de d\u00edas por chunk (opcional, default: 7)\n\n    Returns:\n        pandas.DataFrame: DataFrame con una fila por customer\n    \"\"\"\n    realm_id = get_secret_value('qb_realm_id')\n    access_token = get_secret_value('qb_access_token')\n    minor_version = 75\n    base_url = 'https://sandbox-quickbooks.api.intuit.com'\n    \n    print(\"REFRESCANDO TOKEN\")\n    new_access_token, new_refresh_token = _refrescar_access_token()\n    \n    if new_access_token:\n        access_token = new_access_token  \n        print(f\"Token refrescado con exito\")\n    else:\n        print(\"Error al regrescar el token, puede estar expirado\")\n    \n    start_date_str = kwargs.get('fecha_inicio')\n    end_date_str = kwargs.get('fecha_fin')\n    chunk_days = kwargs.get('chunk_days', 7) \n    \n    # variables de recuperacion\n    resume_mode = kwargs.get('resume_mode', False)  # True para reanudar desde \u00faltimo exitoso\n    retry_failed_chunks = kwargs.get('retry_failed_chunks', False)  # True para reintentar fallos\n    verify_only = kwargs.get('verify_only', False)  # True para solo verificar sin procesar\n    skip_chunks = kwargs.get('skip_chunks', [])  # Lista de n\u00fameros de chunk a omitir\n    force_chunks = kwargs.get('force_chunks', [])  # Lista de n\u00fameros de chunk a forzar reproceso\n    \n    print(f\"CONFIGURACI\u00d3N DE PROCESAMIENTO\")\n    print(f\"Resume mode: {'ACTIVADO' if resume_mode else 'DESACTIVADO'}\")\n    print(f\"Retry failed chunks: {'ACTIVADO' if retry_failed_chunks else 'DESACTIVADO'}\")\n    print(f\"Verify only: {'Solo verificaci\u00f3n' if verify_only else 'Procesamiento normal'}\")\n    print(f\"Skip chunks: {skip_chunks if skip_chunks else 'Ninguno'}\")\n    print(f\"Force chunks: {force_chunks if force_chunks else 'Ninguno'}\")\n    \n    if not start_date_str or not end_date_str:\n        raise ValueError(\"Se requieren los par\u00e1metros 'fecha_inicio' y 'fecha_fin' en formato YYYY-MM-DD\")\n    \n    try:\n        start_date = datetime.strptime(start_date_str, '%Y-%m-%d').date()\n        end_date = datetime.strptime(end_date_str, '%Y-%m-%d').date()\n    except ValueError as e:\n        raise ValueError(f\"Formato de fecha inv\u00e1lido. Use YYYY-MM-DD: {e}\")\n    \n    if start_date > end_date:\n        raise ValueError(\"La fecha de inicio debe ser menor o igual a la fecha de fin\")\n    \n    # timestamp de ingesta\n    ingested_at_utc = kwargs.get('execution_date', datetime.utcnow())\n    \n    # Convertir a string si es datetime\n    if isinstance(ingested_at_utc, datetime):\n        ingested_at_utc_str = ingested_at_utc.isoformat() + 'Z'\n    else:\n        ingested_at_utc_str = str(ingested_at_utc)\n    \n    print(f'INICIO DEL BACKFILL DE QB customers')\n    print(f'Rango completo: {start_date_str} a {end_date_str}')\n    print(f'Chunk size: {chunk_days} d\u00edas')\n    print(f'Ingested at: {ingested_at_utc_str}')\n    \n    chunks = []\n    current_date = start_date\n    chunk_number = 1\n    \n    while current_date <= end_date:\n        chunk_end = min(current_date + timedelta(days=chunk_days - 1), end_date)\n        chunks.append({\n            'chunk_number': chunk_number,\n            'start_date': current_date,\n            'end_date': chunk_end,\n            'start_date_str': current_date.strftime('%Y-%m-%d'),\n            'end_date_str': chunk_end.strftime('%Y-%m-%d')\n        })\n        current_date = chunk_end + timedelta(days=1)\n        chunk_number += 1\n    \n    print(f'Total de chunks a procesar: {len(chunks)}')\n    \n    # tracking de progreso y recuperacion\n    progress_tracker = {\n        'run_id': f\"{start_date_str}_{end_date_str}_{chunk_days}d_{ingested_at_utc_str.split('T')[0]}\",\n        'total_chunks': len(chunks),\n        'completed_chunks': [],\n        'failed_chunks': [],\n        'skipped_chunks': skip_chunks,\n        'processing_start': datetime.utcnow().isoformat() + 'Z'\n    }\n    \n    # resume/retry\n    chunks_to_process = []\n    for chunk in chunks:\n        chunk_num = chunk['chunk_number']\n        \n        # Verificar si saltar este chunk\n        if chunk_num in skip_chunks:\n            print(f\"Saltando chunk {chunk_num} (en skip_chunks)\")\n            progress_tracker['skipped_chunks'].append(chunk_num)\n            continue\n            \n        # Verificar si forzar reproceso\n        if force_chunks and chunk_num in force_chunks:\n            print(f\"Forzando reproceso del chunk {chunk_num}\")\n            chunks_to_process.append(chunk)\n            continue\n        \n        # modo verify_only, solo mostrar qu\u00e9 se har\u00eda\n        if verify_only:\n            print(f\"[VERIFY] Chunk {chunk_num}: {chunk['start_date_str']} a {chunk['end_date_str']}\")\n            continue\n            \n        if resume_mode:\n            print(f\"[RESUME] Verificando chunk {chunk_num}...\")\n        \n        chunks_to_process.append(chunk)\n    \n    if verify_only:\n        print(f\"\\nVERIFICACI\u00d3N COMPLETADA\")\n        print(f\"Total chunks definidos: {len(chunks)}\")\n        print(f\"Chunks a saltar: {len(skip_chunks)}\")\n        print(f\"Chunks a forzar: {len(force_chunks)}\")\n        print(f\"Chunks que se procesar\u00edan: {len(chunks_to_process)}\")\n        return pd.DataFrame()  # DataFrame vac\u00edo en modo verificaci\u00f3n\n    \n    print(f\"RESUMEN DE PROCESAMIENTO:\")\n    print(f\"  Total chunks definidos: {len(chunks)}\")\n    print(f\"  Chunks a procesar: {len(chunks_to_process)}\")\n    print(f\"  Chunks a saltar: {len(skip_chunks)}\")\n    print(f\"  Chunks forzados: {len(force_chunks) if force_chunks else 0}\")\n    \n    # Lista para almacenar todas las filas del DataFrame\n    all_rows = []\n    total_customers = 0\n    total_pages = 0\n    processed_chunks_count = 0\n    \n    for chunk in chunks_to_process:\n        chunk_start_time = time.time()\n        processed_chunks_count += 1\n        \n        print(f'\\nPROCESANDO CHUNK {chunk[\"chunk_number\"]}/{len(chunks)} ({processed_chunks_count}/{len(chunks_to_process)} a procesar)')\n        print(f'Fechas procesadas: {chunk[\"start_date_str\"]} a {chunk[\"end_date_str\"]}')\n        \n        try:\n            # query para el chunk actual\n            start_utc = f\"{chunk['start_date_str']}T00:00:00Z\"\n            end_utc = f\"{chunk['end_date_str']}T23:59:59Z\"\n            \n            query = f\"select * from Customer where MetaData.LastUpdatedTime >= '{start_utc}' and MetaData.LastUpdatedTime <= '{end_utc}'\"\n            \n            # p\u00e1ginas para este chunk\n            chunk_customers = 0\n            chunk_pages = 0\n            max_results = 100\n            start_position = 1\n            page_number = 1\n            \n            while True:\n                page_start_time = time.time()\n                \n                # p\u00e1gina actual\n                data = _fetch_qb_data(\n                    realm_id=realm_id,\n                    access_token=access_token,\n                    query=query,\n                    base_url=base_url,\n                    minor_version=minor_version,\n                    start_position=start_position,\n                    max_results=max_results\n                )\n                \n                if not data or 'QueryResponse' not in data:\n                    print(f'  No se encontraron m\u00e1s datos en p\u00e1gina {page_number}')\n                    break\n                    \n                query_response = data['QueryResponse']\n                \n                if 'Customer' not in query_response:\n                    print(f'  No se encontraron customers en p\u00e1gina {page_number}')\n                    break\n                    \n                customers = query_response['Customer']\n                page_end_time = time.time()\n                page_duration = page_end_time - page_start_time\n                \n                # URL completa de la llamada API\n                paginated_query = f\"{query} STARTPOSITION {start_position} MAXRESULTS {max_results}\"\n                full_api_url = f\"{base_url.rstrip('/')}/v3/company/{realm_id}/query?query={paginated_query}&minorversion={minor_version}\"\n                \n                # Metadatos comunes de la p\u00e1gina\n                page_common_data = {\n                    'ingested_at_utc': ingested_at_utc_str,\n                    'extract_window_start_utc': start_utc,\n                    'extract_window_end_utc': end_utc,\n                    'page_number': page_number,\n                    'page_size': len(customers),\n                    'request_payload': json.dumps({\n                        'full_api_url': full_api_url,\n                        'method': 'GET',\n                        'headers': {\n                            'Authorization': 'Bearer [HIDDEN]',\n                            'Accept': 'application/json',\n                            'Content-Type': 'text/plain'\n                        },\n                        'query_parameters': {\n                            'query': paginated_query,\n                            'minorversion': minor_version\n                        },\n                        'base_url': base_url,\n                        'realm_id': realm_id,\n                        'original_query': query\n                    })\n                }\n                \n                # Procesar cada customer en la p\u00e1gina y agregar al DataFrame\n                for customer in customers:\n                    row = {\n                        'id': customer.get('Id'),\n                        'payload': json.dumps(customer),\n                        **page_common_data\n                    }\n                    all_rows.append(row)\n                \n                chunk_customers += len(customers)\n                chunk_pages += 1\n                \n                print(f'  P\u00e1gina {page_number}: {len(customers)} customers en {page_duration:.2f}s')\n                \n                # si recibimos menos registros de los solicitados, es la \u00faltima p\u00e1gina\n                if len(customers) < max_results:\n                    break\n                    \n                # avanzar a la siguiente p\u00e1gina\n                start_position += max_results\n                page_number += 1\n            \n            chunk_end_time = time.time()\n            chunk_duration = chunk_end_time - chunk_start_time\n            \n            # Actualizar totales\n            total_customers += chunk_customers\n            total_pages += chunk_pages\n            \n            # Marcar chunk como completado exitosamente\n            progress_tracker['completed_chunks'].append(chunk['chunk_number'])\n            \n            # LOGS DEL TRAMO COMPLETADO\n            print(f'\\nCHUNK {chunk[\"chunk_number\"]} COMPLETADO')\n            print(f'Fechas procesadas: {chunk[\"start_date_str\"]} a {chunk[\"end_date_str\"]}')\n            print(f'P\u00e1ginas le\u00eddas: {chunk_pages}')\n            print(f'Filas insertadas: {chunk_customers}')\n            print(f'Duraci\u00f3n total del chunk: {chunk_duration:.2f} segundos')\n            print(f'Promedio por p\u00e1gina: {chunk_duration/max(chunk_pages, 1):.2f} segundos')\n            print(f'Velocidad de ingesta: {chunk_customers/max(chunk_duration, 0.1):.2f} customers/segundo')\n            print(f'Progreso general: {chunk[\"chunk_number\"]}/{len(chunks)} chunks ({(chunk[\"chunk_number\"]/len(chunks)*100):.1f}%)')\n            print(f'Total acumulado hasta ahora: {total_customers} customers en {total_pages} p\u00e1ginas')\n            print('-' * 60)\n        \n        except Exception as chunk_error:\n            # Manejo de errores de chunk completo\n            chunk_end_time = time.time()\n            chunk_duration = chunk_end_time - chunk_start_time\n            \n            print(f'\\nERROR EN CHUNK {chunk[\"chunk_number\"]}')\n            print(f'Fechas afectadas: {chunk[\"start_date_str\"]} a {chunk[\"end_date_str\"]}')\n            print(f'Error: {str(chunk_error)}')\n            print(f'Duraci\u00f3n antes del error: {chunk_duration:.2f} segundos')\n            \n            # Marcar chunk como fallido\n            progress_tracker['failed_chunks'].append({\n                'chunk_number': chunk['chunk_number'],\n                'date_range': f\"{chunk['start_date_str']} a {chunk['end_date_str']}\",\n                'error': str(chunk_error),\n                'duration': chunk_duration\n            })\n            \n            # Decidir si continuar o fallar completamente\n            if retry_failed_chunks:\n                print(f\"Chunk fallido marcado para reintento posterior\")\n            else:\n                print(f\"Continuando con siguiente chunk (chunk fallido omitido)\")\n            \n            # Continuar con el siguiente chunk\n            continue\n    \n    print(f'\\nBACKFILL COMPLETADO')\n    print(f'Total chunks procesados: {len(chunks)}')\n    print(f'Total p\u00e1ginas: {total_pages}')\n    print(f'Rango procesado: {start_date_str} a {end_date_str}')\n    \n    # Crear DataFrame final\n    df = pd.DataFrame(all_rows)\n    \n    # eliminar duplicados:  esto puede ocurrir cuando customers caen en dos rangos de fecha de consultas\n    if not df.empty:\n        filas_antes = len(df)\n        df = df.drop_duplicates(subset=['id'], keep='first')  # Mantener la primera ocurrencia\n        filas_despues = len(df)\n        duplicados_eliminados = filas_antes - filas_despues\n    \n    # reordenar columnas en el orden deseado \n    if not df.empty:\n        column_order = [\n            'id',\n            'payload', \n            'ingested_at_utc',\n            'extract_window_start_utc',\n            'extract_window_end_utc',\n            'page_number',\n            'page_size',\n            'request_payload'\n        ]\n\n        available_columns = [col for col in column_order if col in df.columns]\n        df = df[available_columns]\n\n    print(f'Total customers(luego de eliminar duplicados): {len(df)}')\n    print(f\"\\nDataFrame creado con {len(df)} customers\")\n    \n    return df\n\n\n@test\ndef test_output(output, *args) -> None:\n    \"\"\"\n    Template code for testing the output of the block.\n    \"\"\"\n    assert output is not None, 'The output is undefined'\n", "file_path": "data_loaders/ingest_qb_customers.py", "language": "python", "type": "data_loader", "uuid": "ingest_qb_customers"}, "data_loaders/ingest_qb_invoices.py:data_loader:python:ingest qb invoices": {"content": "import requests\nimport json\nimport pandas as pd\nfrom datetime import datetime, timedelta\nfrom mage_ai.data_preparation.shared.secrets import get_secret_value\nimport time\n\n\nif 'data_loader' not in globals():\n    from mage_ai.data_preparation.decorators import data_loader\nif 'test' not in globals():\n    from mage_ai.data_preparation.decorators import test\n\ndef _refrescar_access_token():\n\n    refresh_token = get_secret_value('qb_refresh_token')\n    client_id = get_secret_value('qb_client_id')\n    client_secret = get_secret_value('qb_client_secret')\n    \n    url_base = 'https://oauth.platform.intuit.com/oauth2/v1/tokens/bearer'\n\n    headers = {\n        'Content-Type': 'application/x-www-form-urlencoded',\n        'Accept': 'application/json'\n    }\n    \n    data = {\n        'grant_type': 'refresh_token',\n        'refresh_token': refresh_token,\n        'client_id': client_id,\n        'client_secret': client_secret\n    }\n    \n    try:\n        print('Post para refrescar token')\n        response = requests.post(url_base, headers=headers, data=data, timeout=60)\n        response.raise_for_status()\n        token_data = response.json()\n        new_access_token = token_data.get('access_token')\n        new_refresh_token = token_data.get('refresh_token')\n\n        if new_access_token:\n            print('Exito al refrescar token')\n            return new_access_token, new_refresh_token\n        else:\n            raise ValueError(\"Error al solicitar nuevo token\")\n            \n    except requests.exceptions.RequestException as e:\n        print(f'Error al refrescar token: {e}')\n        return None, None\n    except json.JSONDecodeError as e:\n        print(f'Error al decodificar respuesta JSON: {e}')\n        return None, None\n\ndef _fetch_qb_data(realm_id, access_token, query, base_url, minor_version, start_position=1, max_results=1000):\n\n    if not base_url or not minor_version:\n        raise ValueError(\"Se requiere una URL base y el minor version\")    \n\n    headers = {\n        'Authorization': f'Bearer {access_token}',\n        'Accept': 'application/json',\n        'Content-Type': 'text/plain'\n    }\n\n    # paginaci\u00f3n \n    paginated_query = f\"{query} STARTPOSITION {start_position} MAXRESULTS {max_results}\"\n\n    params = {\n        'query': paginated_query,\n        'minorversion': minor_version\n    }\n\n    url = f\"{base_url.rstrip('/')}/v3/company/{realm_id}/query\"\n    \n    #  reintentos con backoff exponencial\n    max_retries = 5\n    base_timeout = 60\n    base_delay = 1  # delay inicial para backoff exponencial\n    \n    # circuit breaker\n    consecutive_failures = getattr(_fetch_qb_data, '_consecutive_failures', 0)\n    circuit_open = getattr(_fetch_qb_data, '_circuit_open_until', 0)\n    \n    # Verificar si circuit breaker est\u00e1 abierto\n    current_time = time.time()\n    if current_time < circuit_open:\n        remaining_time = int(circuit_open - current_time)\n        print(f'CIRCUIT BREAKER ABIERTO - Esperando {remaining_time}s m\u00e1s antes de reintentar')\n        time.sleep(min(remaining_time, 30))  # Esperar m\u00e1ximo 30s en esta llamada\n        return None\n    \n    for attempt in range(max_retries):\n        # timeout incremental\n        current_timeout = base_timeout + (attempt * 30)\n        # backoff exponential delay (solo despu\u00e9s del primer intento)\n        if attempt > 0:\n            delay = base_delay * (2 ** (attempt - 1))  # 1s, 2s, 4s, 8s, 16s\n            print(f'Backoff exponencial: esperando {delay}s antes del intento {attempt + 1}')\n            time.sleep(delay)\n        \n        try:\n            print(f'Intento {attempt + 1}/{max_retries} - Request al API')\n            print(f'URL: {base_url}')\n            print(f'Query: {paginated_query}')\n            print(f'Posici\u00f3n: {start_position}, M\u00e1ximo: {max_results}')\n            print(f'Timeout: {current_timeout}s')\n            print(f'Fallos consecutivos: {consecutive_failures}')\n            \n            response = requests.get(url, headers=headers, params=params, timeout=current_timeout)\n            \n            # manejo de rate limits\n            if response.status_code == 429:  # Too Many Requests\n                retry_after = int(response.headers.get('Retry-After', 60))\n                print(f'RATE LIMIT EXCEDIDO - Esperando {retry_after}s (HTTP 429)')\n                time.sleep(retry_after)\n                continue  # Reintentar sin contar como fallo\n            \n            if response.status_code == 401:\n                print('Token expirado, refrescando...')\n                new_access_token, new_refresh_token = _refrescar_access_token()\n                \n                if new_access_token:\n                    headers['Authorization'] = f'Bearer {new_access_token}'\n                    print('Token refrescado, reintentando...')\n                    response = requests.get(url, headers=headers, params=params, timeout=current_timeout)\n                else:\n                    raise ValueError(\"Error cr\u00edtico: No se pudo refrescar el token\")\n            \n            response.raise_for_status()\n            data = response.json()\n            \n            # \u00c9XITO - Resetear circuit breaker\n            _fetch_qb_data._consecutive_failures = 0\n            _fetch_qb_data._circuit_open_until = 0\n            \n            print(f'Datos recibidos exitosamente en intento {attempt + 1}')\n            print(f'P\u00e1gina desde posici\u00f3n {start_position} obtenida correctamente')\n            return data\n            \n        except requests.exceptions.Timeout as e:\n            consecutive_failures += 1\n            print(f'TIMEOUT en intento {attempt + 1} despu\u00e9s de {current_timeout}s: {e}')\n            _handle_failure(attempt, max_retries, consecutive_failures, 'TIMEOUT')\n                \n        except requests.exceptions.ConnectionError as e:\n            consecutive_failures += 1\n            print(f'ERROR DE CONEXI\u00d3N en intento {attempt + 1}: {e}')\n            _handle_failure(attempt, max_retries, consecutive_failures, 'CONNECTION_ERROR')\n                \n        except requests.exceptions.RequestException as e:\n            consecutive_failures += 1\n            print(f'ERROR DE REQUEST en intento {attempt + 1}: {e}')\n            _handle_failure(attempt, max_retries, consecutive_failures, 'REQUEST_ERROR')\n                \n        except json.JSONDecodeError as e:\n            consecutive_failures += 1\n            print(f'ERROR DE JSON en intento {attempt + 1}: {e}')\n            _handle_failure(attempt, max_retries, consecutive_failures, 'JSON_ERROR')\n                \n        except Exception as e:\n            consecutive_failures += 1\n            print(f'ERROR INESPERADO en intento {attempt + 1}: {e}')\n            _handle_failure(attempt, max_retries, consecutive_failures, 'UNEXPECTED_ERROR')\n    \n    # todos los reintentos fallaron\n    _fetch_qb_data._consecutive_failures = consecutive_failures\n    _activate_circuit_breaker(consecutive_failures)\n    print(f'FALLO TOTAL: {max_retries} intentos agotados. Fallos consecutivos: {consecutive_failures}')\n    return None\n\ndef _handle_failure(attempt, max_retries, consecutive_failures, error_type):\n    if attempt == max_retries - 1:  # \u00daltimo intento\n        print(f'\u00daLTIMO INTENTO FALLADO - Tipo: {error_type}')\n        print(f'Fallos consecutivos acumulados: {consecutive_failures}')\n    else:\n        next_delay = 1 * (2 ** attempt)  # Pr\u00f3ximo delay exponencial\n        print(f'Preparando reintento con backoff exponencial de {next_delay}s')\n\ndef _activate_circuit_breaker(consecutive_failures):\n    if consecutive_failures >= 10:\n        circuit_open_time = 300\n        print('CIRCUIT BREAKER CR\u00cdTICO: 10+ fallos consecutivos - Pausando 5 minutos')\n    elif consecutive_failures >= 5:\n        circuit_open_time = 60\n        print('CIRCUIT BREAKER ACTIVADO: 5+ fallos consecutivos - Pausando 1 minuto')\n    else:\n        return  \n    \n    _fetch_qb_data._circuit_open_until = time.time() + circuit_open_time\n    _fetch_qb_data._consecutive_failures = consecutive_failures\n\n@data_loader\ndef load_data(*args, **kwargs):\n    \"\"\"\n    Template code for loading data from any source.\n    \n    Args:\n        fecha_inicio (str): Fecha de inicio en formato YYYY-MM-DD (requerido para backfill)\n        fecha_fin (str): Fecha de fin en formato YYYY-MM-DD (requerido para backfill)\n        chunk_days (int): N\u00famero de d\u00edas por chunk (opcional, default: 7)\n\n    Returns:\n        pandas.DataFrame: DataFrame con una fila por invoice\n    \"\"\"\n    realm_id = get_secret_value('qb_realm_id')\n    access_token = get_secret_value('qb_access_token')\n    minor_version = 75\n    base_url = 'https://sandbox-quickbooks.api.intuit.com'\n    \n    print(\"REFRESCANDO TOKEN\")\n    new_access_token, new_refresh_token = _refrescar_access_token()\n    \n    if new_access_token:\n        access_token = new_access_token  \n        print(f\"Token refrescado con exito\")\n    else:\n        print(\"Error al regrescar el token, puede estar expirado\")\n    \n    start_date_str = kwargs.get('fecha_inicio')\n    end_date_str = kwargs.get('fecha_fin')\n    chunk_days = kwargs.get('chunk_days', 7) \n    \n    # variables de recuperacion\n    resume_mode = kwargs.get('resume_mode', False)  # True para reanudar desde \u00faltimo exitoso\n    retry_failed_chunks = kwargs.get('retry_failed_chunks', False)  # True para reintentar fallos\n    verify_only = kwargs.get('verify_only', False)  # True para solo verificar sin procesar\n    skip_chunks = kwargs.get('skip_chunks', [])  # Lista de n\u00fameros de chunk a omitir\n    force_chunks = kwargs.get('force_chunks', [])  # Lista de n\u00fameros de chunk a forzar reproceso\n    \n    print(f\"CONFIGURACI\u00d3N DE PROCESAMIENTO\")\n    print(f\"Resume mode: {'ACTIVADO' if resume_mode else 'DESACTIVADO'}\")\n    print(f\"Retry failed chunks: {'ACTIVADO' if retry_failed_chunks else 'DESACTIVADO'}\")\n    print(f\"Verify only: {'Solo verificaci\u00f3n' if verify_only else 'Procesamiento normal'}\")\n    print(f\"Skip chunks: {skip_chunks if skip_chunks else 'Ninguno'}\")\n    print(f\"Force chunks: {force_chunks if force_chunks else 'Ninguno'}\")\n    \n    if not start_date_str or not end_date_str:\n        raise ValueError(\"Se requieren los par\u00e1metros 'fecha_inicio' y 'fecha_fin' en formato YYYY-MM-DD\")\n    \n    try:\n        start_date = datetime.strptime(start_date_str, '%Y-%m-%d').date()\n        end_date = datetime.strptime(end_date_str, '%Y-%m-%d').date()\n    except ValueError as e:\n        raise ValueError(f\"Formato de fecha inv\u00e1lido. Use YYYY-MM-DD: {e}\")\n    \n    if start_date > end_date:\n        raise ValueError(\"La fecha de inicio debe ser menor o igual a la fecha de fin\")\n    \n    # timestamp de ingesta\n    ingested_at_utc = kwargs.get('execution_date', datetime.utcnow())\n    \n    # Convertir a string si es datetime\n    if isinstance(ingested_at_utc, datetime):\n        ingested_at_utc_str = ingested_at_utc.isoformat() + 'Z'\n    else:\n        ingested_at_utc_str = str(ingested_at_utc)\n    \n    print(f'INICIO DEL BACKFILL DE QB INVOICES')\n    print(f'Rango completo: {start_date_str} a {end_date_str}')\n    print(f'Chunk size: {chunk_days} d\u00edas')\n    print(f'Ingested at: {ingested_at_utc_str}')\n    \n    chunks = []\n    current_date = start_date\n    chunk_number = 1\n    \n    while current_date <= end_date:\n        chunk_end = min(current_date + timedelta(days=chunk_days - 1), end_date)\n        chunks.append({\n            'chunk_number': chunk_number,\n            'start_date': current_date,\n            'end_date': chunk_end,\n            'start_date_str': current_date.strftime('%Y-%m-%d'),\n            'end_date_str': chunk_end.strftime('%Y-%m-%d')\n        })\n        current_date = chunk_end + timedelta(days=1)\n        chunk_number += 1\n    \n    print(f'Total de chunks a procesar: {len(chunks)}')\n    \n    # tracking de progreso y recuperacion\n    progress_tracker = {\n        'run_id': f\"{start_date_str}_{end_date_str}_{chunk_days}d_{ingested_at_utc_str.split('T')[0]}\",\n        'total_chunks': len(chunks),\n        'completed_chunks': [],\n        'failed_chunks': [],\n        'skipped_chunks': skip_chunks,\n        'processing_start': datetime.utcnow().isoformat() + 'Z'\n    }\n    \n    # resume/retry\n    chunks_to_process = []\n    for chunk in chunks:\n        chunk_num = chunk['chunk_number']\n        \n        # Verificar si saltar este chunk\n        if chunk_num in skip_chunks:\n            print(f\"Saltando chunk {chunk_num} (en skip_chunks)\")\n            progress_tracker['skipped_chunks'].append(chunk_num)\n            continue\n            \n        # Verificar si forzar reproceso\n        if force_chunks and chunk_num in force_chunks:\n            print(f\"Forzando reproceso del chunk {chunk_num}\")\n            chunks_to_process.append(chunk)\n            continue\n        \n        # modo verify_only, solo mostrar qu\u00e9 se har\u00eda\n        if verify_only:\n            print(f\"[VERIFY] Chunk {chunk_num}: {chunk['start_date_str']} a {chunk['end_date_str']}\")\n            continue\n            \n        if resume_mode:\n            print(f\"[RESUME] Verificando chunk {chunk_num}...\")\n        \n        chunks_to_process.append(chunk)\n    \n    if verify_only:\n        print(f\"\\nVERIFICACI\u00d3N COMPLETADA\")\n        print(f\"Total chunks definidos: {len(chunks)}\")\n        print(f\"Chunks a saltar: {len(skip_chunks)}\")\n        print(f\"Chunks a forzar: {len(force_chunks)}\")\n        print(f\"Chunks que se procesar\u00edan: {len(chunks_to_process)}\")\n        return pd.DataFrame()  # DataFrame vac\u00edo en modo verificaci\u00f3n\n    \n    print(f\"RESUMEN DE PROCESAMIENTO:\")\n    print(f\"  Total chunks definidos: {len(chunks)}\")\n    print(f\"  Chunks a procesar: {len(chunks_to_process)}\")\n    print(f\"  Chunks a saltar: {len(skip_chunks)}\")\n    print(f\"  Chunks forzados: {len(force_chunks) if force_chunks else 0}\")\n    \n    # Lista para almacenar todas las filas del DataFrame\n    all_rows = []\n    total_invoices = 0\n    total_pages = 0\n    processed_chunks_count = 0\n    \n    for chunk in chunks_to_process:\n        chunk_start_time = time.time()\n        processed_chunks_count += 1\n        \n        print(f'\\nPROCESANDO CHUNK {chunk[\"chunk_number\"]}/{len(chunks)} ({processed_chunks_count}/{len(chunks_to_process)} a procesar)')\n        print(f'Fechas procesadas: {chunk[\"start_date_str\"]} a {chunk[\"end_date_str\"]}')\n        \n        try:\n            # query para el chunk actual\n            start_utc = f\"{chunk['start_date_str']}T00:00:00Z\"\n            end_utc = f\"{chunk['end_date_str']}T23:59:59Z\"\n            \n            query = f\"select * from Invoice where TxnDate >= '{start_utc}' and TxnDate <= '{end_utc}'\"\n            \n            # p\u00e1ginas para este chunk\n            chunk_invoices = 0\n            chunk_pages = 0\n            max_results = 100\n            start_position = 1\n            page_number = 1\n            \n            while True:\n                page_start_time = time.time()\n                \n                # p\u00e1gina actual\n                data = _fetch_qb_data(\n                    realm_id=realm_id,\n                    access_token=access_token,\n                    query=query,\n                    base_url=base_url,\n                    minor_version=minor_version,\n                    start_position=start_position,\n                    max_results=max_results\n                )\n                \n                if not data or 'QueryResponse' not in data:\n                    print(f'  No se encontraron m\u00e1s datos en p\u00e1gina {page_number}')\n                    break\n                    \n                query_response = data['QueryResponse']\n                \n                if 'Invoice' not in query_response:\n                    print(f'  No se encontraron invoices en p\u00e1gina {page_number}')\n                    break\n                    \n                invoices = query_response['Invoice']\n                page_end_time = time.time()\n                page_duration = page_end_time - page_start_time\n                \n                # URL completa de la llamada API\n                paginated_query = f\"{query} STARTPOSITION {start_position} MAXRESULTS {max_results}\"\n                full_api_url = f\"{base_url.rstrip('/')}/v3/company/{realm_id}/query?query={paginated_query}&minorversion={minor_version}\"\n                \n                # Metadatos comunes de la p\u00e1gina\n                page_common_data = {\n                    'ingested_at_utc': ingested_at_utc_str,\n                    'extract_window_start_utc': start_utc,\n                    'extract_window_end_utc': end_utc,\n                    'page_number': page_number,\n                    'page_size': len(invoices),\n                    'request_payload': json.dumps({\n                        'full_api_url': full_api_url,\n                        'method': 'GET',\n                        'headers': {\n                            'Authorization': 'Bearer [HIDDEN]',\n                            'Accept': 'application/json',\n                            'Content-Type': 'text/plain'\n                        },\n                        'query_parameters': {\n                            'query': paginated_query,\n                            'minorversion': minor_version\n                        },\n                        'base_url': base_url,\n                        'realm_id': realm_id,\n                        'original_query': query\n                    })\n                }\n                \n                # Procesar cada invoice en la p\u00e1gina y agregar al DataFrame\n                for invoice in invoices:\n                    row = {\n                        'id': invoice.get('Id'),\n                        'payload': json.dumps(invoice),\n                        **page_common_data\n                    }\n                    all_rows.append(row)\n                \n                chunk_invoices += len(invoices)\n                chunk_pages += 1\n                \n                print(f'  P\u00e1gina {page_number}: {len(invoices)} invoices en {page_duration:.2f}s')\n                \n                # si recibimos menos registros de los solicitados, es la \u00faltima p\u00e1gina\n                if len(invoices) < max_results:\n                    break\n                    \n                # avanzar a la siguiente p\u00e1gina\n                start_position += max_results\n                page_number += 1\n            \n            chunk_end_time = time.time()\n            chunk_duration = chunk_end_time - chunk_start_time\n            \n            # Actualizar totales\n            total_invoices += chunk_invoices\n            total_pages += chunk_pages\n            \n            # Marcar chunk como completado exitosamente\n            progress_tracker['completed_chunks'].append(chunk['chunk_number'])\n            \n            # LOGS DEL TRAMO COMPLETADO\n            print(f'\\nCHUNK {chunk[\"chunk_number\"]} COMPLETADO')\n            print(f'Fechas procesadas: {chunk[\"start_date_str\"]} a {chunk[\"end_date_str\"]}')\n            print(f'P\u00e1ginas le\u00eddas: {chunk_pages}')\n            print(f'Filas insertadas: {chunk_invoices}')\n            print(f'Duraci\u00f3n total del chunk: {chunk_duration:.2f} segundos')\n            print(f'Promedio por p\u00e1gina: {chunk_duration/max(chunk_pages, 1):.2f} segundos')\n            print(f'Velocidad de ingesta: {chunk_invoices/max(chunk_duration, 0.1):.2f} invoices/segundo')\n            print(f'Progreso general: {chunk[\"chunk_number\"]}/{len(chunks)} chunks ({(chunk[\"chunk_number\"]/len(chunks)*100):.1f}%)')\n            print(f'Total acumulado hasta ahora: {total_invoices} invoices en {total_pages} p\u00e1ginas')\n            print('-' * 60)\n        \n        except Exception as chunk_error:\n            # Manejo de errores de chunk completo\n            chunk_end_time = time.time()\n            chunk_duration = chunk_end_time - chunk_start_time\n            \n            print(f'\\nERROR EN CHUNK {chunk[\"chunk_number\"]}')\n            print(f'Fechas afectadas: {chunk[\"start_date_str\"]} a {chunk[\"end_date_str\"]}')\n            print(f'Error: {str(chunk_error)}')\n            print(f'Duraci\u00f3n antes del error: {chunk_duration:.2f} segundos')\n            \n            # Marcar chunk como fallido\n            progress_tracker['failed_chunks'].append({\n                'chunk_number': chunk['chunk_number'],\n                'date_range': f\"{chunk['start_date_str']} a {chunk['end_date_str']}\",\n                'error': str(chunk_error),\n                'duration': chunk_duration\n            })\n            \n            # Decidir si continuar o fallar completamente\n            if retry_failed_chunks:\n                print(f\"Chunk fallido marcado para reintento posterior\")\n            else:\n                print(f\"Continuando con siguiente chunk (chunk fallido omitido)\")\n            \n            # Continuar con el siguiente chunk\n            continue\n    \n    print(f'\\nBACKFILL COMPLETADO')\n    print(f'Total chunks procesados: {len(chunks)}')\n    print(f'Total p\u00e1ginas: {total_pages}')\n    print(f'Rango procesado: {start_date_str} a {end_date_str}')\n    \n    # Crear DataFrame final\n    df = pd.DataFrame(all_rows)\n    \n    # eliminar duplicados:  esto puede ocurrir cuando invoices caen en dos rangos de fecha de consultas\n    if not df.empty:\n        filas_antes = len(df)\n        df = df.drop_duplicates(subset=['id'], keep='first')  # Mantener la primera ocurrencia\n        filas_despues = len(df)\n        duplicados_eliminados = filas_antes - filas_despues\n    \n    # reordenar columnas en el orden deseado \n    if not df.empty:\n        column_order = [\n            'id',\n            'payload', \n            'ingested_at_utc',\n            'extract_window_start_utc',\n            'extract_window_end_utc',\n            'page_number',\n            'page_size',\n            'request_payload'\n        ]\n\n        available_columns = [col for col in column_order if col in df.columns]\n        df = df[available_columns]\n\n    print(f'Total invoices(luego de eliminar duplicados): {len(df)}')\n    print(f\"\\nDataFrame creado con {len(df)} invoices\")\n    \n    return df\n\n\n@test\ndef test_output(output, *args) -> None:\n    \"\"\"\n    Template code for testing the output of the block.\n    \"\"\"\n    assert output is not None, 'The output is undefined'\n", "file_path": "data_loaders/ingest_qb_invoices.py", "language": "python", "type": "data_loader", "uuid": "ingest_qb_invoices"}, "data_loaders/ingest_qb_items.py:data_loader:python:ingest qb items": {"content": "import requests\nimport json\nimport pandas as pd\nfrom datetime import datetime, timedelta\nfrom mage_ai.data_preparation.shared.secrets import get_secret_value\nimport time\n\n\nif 'data_loader' not in globals():\n    from mage_ai.data_preparation.decorators import data_loader\nif 'test' not in globals():\n    from mage_ai.data_preparation.decorators import test\n\ndef _refrescar_access_token():\n\n    refresh_token = get_secret_value('qb_refresh_token')\n    client_id = get_secret_value('qb_client_id')\n    client_secret = get_secret_value('qb_client_secret')\n    \n    url_base = 'https://oauth.platform.intuit.com/oauth2/v1/tokens/bearer'\n\n    headers = {\n        'Content-Type': 'application/x-www-form-urlencoded',\n        'Accept': 'application/json'\n    }\n    \n    data = {\n        'grant_type': 'refresh_token',\n        'refresh_token': refresh_token,\n        'client_id': client_id,\n        'client_secret': client_secret\n    }\n    \n    try:\n        print('Post para refrescar token')\n        response = requests.post(url_base, headers=headers, data=data, timeout=60)\n        response.raise_for_status()\n        token_data = response.json()\n        new_access_token = token_data.get('access_token')\n        new_refresh_token = token_data.get('refresh_token')\n\n        if new_access_token:\n            print('Exito al refrescar token')\n            return new_access_token, new_refresh_token\n        else:\n            raise ValueError(\"Error al solicitar nuevo token\")\n            \n    except requests.exceptions.RequestException as e:\n        print(f'Error al refrescar token: {e}')\n        return None, None\n    except json.JSONDecodeError as e:\n        print(f'Error al decodificar respuesta JSON: {e}')\n        return None, None\n\ndef _fetch_qb_data(realm_id, access_token, query, base_url, minor_version, start_position=1, max_results=1000):\n\n    if not base_url or not minor_version:\n        raise ValueError(\"Se requiere una URL base y el minor version\")    \n\n    headers = {\n        'Authorization': f'Bearer {access_token}',\n        'Accept': 'application/json',\n        'Content-Type': 'text/plain'\n    }\n\n    # paginaci\u00f3n \n    paginated_query = f\"{query} STARTPOSITION {start_position} MAXRESULTS {max_results}\"\n\n    params = {\n        'query': paginated_query,\n        'minorversion': minor_version\n    }\n\n    url = f\"{base_url.rstrip('/')}/v3/company/{realm_id}/query\"\n    \n    #  reintentos con backoff exponencial\n    max_retries = 5\n    base_timeout = 60\n    base_delay = 1  # delay inicial para backoff exponencial\n    \n    # circuit breaker\n    consecutive_failures = getattr(_fetch_qb_data, '_consecutive_failures', 0)\n    circuit_open = getattr(_fetch_qb_data, '_circuit_open_until', 0)\n    \n    # Verificar si circuit breaker est\u00e1 abierto\n    current_time = time.time()\n    if current_time < circuit_open:\n        remaining_time = int(circuit_open - current_time)\n        print(f'CIRCUIT BREAKER ABIERTO - Esperando {remaining_time}s m\u00e1s antes de reintentar')\n        time.sleep(min(remaining_time, 30))  # Esperar m\u00e1ximo 30s en esta llamada\n        return None\n    \n    for attempt in range(max_retries):\n        # timeout incremental\n        current_timeout = base_timeout + (attempt * 30)\n        # backoff exponential delay (solo despu\u00e9s del primer intento)\n        if attempt > 0:\n            delay = base_delay * (2 ** (attempt - 1))  # 1s, 2s, 4s, 8s, 16s\n            print(f'Backoff exponencial: esperando {delay}s antes del intento {attempt + 1}')\n            time.sleep(delay)\n        \n        try:\n            print(f'Intento {attempt + 1}/{max_retries} - Request al API')\n            print(f'URL: {base_url}')\n            print(f'Query: {paginated_query}')\n            print(f'Posici\u00f3n: {start_position}, M\u00e1ximo: {max_results}')\n            print(f'Timeout: {current_timeout}s')\n            print(f'Fallos consecutivos: {consecutive_failures}')\n            \n            response = requests.get(url, headers=headers, params=params, timeout=current_timeout)\n            \n            # manejo de rate limits\n            if response.status_code == 429:  # Too Many Requests\n                retry_after = int(response.headers.get('Retry-After', 60))\n                print(f'RATE LIMIT EXCEDIDO - Esperando {retry_after}s (HTTP 429)')\n                time.sleep(retry_after)\n                continue  # Reintentar sin contar como fallo\n            \n            if response.status_code == 401:\n                print('Token expirado, refrescando...')\n                new_access_token, new_refresh_token = _refrescar_access_token()\n                \n                if new_access_token:\n                    headers['Authorization'] = f'Bearer {new_access_token}'\n                    print('Token refrescado, reintentando...')\n                    response = requests.get(url, headers=headers, params=params, timeout=current_timeout)\n                else:\n                    raise ValueError(\"Error cr\u00edtico: No se pudo refrescar el token\")\n            \n            response.raise_for_status()\n            data = response.json()\n            \n            # \u00c9XITO - Resetear circuit breaker\n            _fetch_qb_data._consecutive_failures = 0\n            _fetch_qb_data._circuit_open_until = 0\n            \n            print(f'Datos recibidos exitosamente en intento {attempt + 1}')\n            print(f'P\u00e1gina desde posici\u00f3n {start_position} obtenida correctamente')\n            return data\n            \n        except requests.exceptions.Timeout as e:\n            consecutive_failures += 1\n            print(f'TIMEOUT en intento {attempt + 1} despu\u00e9s de {current_timeout}s: {e}')\n            _handle_failure(attempt, max_retries, consecutive_failures, 'TIMEOUT')\n                \n        except requests.exceptions.ConnectionError as e:\n            consecutive_failures += 1\n            print(f'ERROR DE CONEXI\u00d3N en intento {attempt + 1}: {e}')\n            _handle_failure(attempt, max_retries, consecutive_failures, 'CONNECTION_ERROR')\n                \n        except requests.exceptions.RequestException as e:\n            consecutive_failures += 1\n            print(f'ERROR DE REQUEST en intento {attempt + 1}: {e}')\n            _handle_failure(attempt, max_retries, consecutive_failures, 'REQUEST_ERROR')\n                \n        except json.JSONDecodeError as e:\n            consecutive_failures += 1\n            print(f'ERROR DE JSON en intento {attempt + 1}: {e}')\n            _handle_failure(attempt, max_retries, consecutive_failures, 'JSON_ERROR')\n                \n        except Exception as e:\n            consecutive_failures += 1\n            print(f'ERROR INESPERADO en intento {attempt + 1}: {e}')\n            _handle_failure(attempt, max_retries, consecutive_failures, 'UNEXPECTED_ERROR')\n    \n    # todos los reintentos fallaron\n    _fetch_qb_data._consecutive_failures = consecutive_failures\n    _activate_circuit_breaker(consecutive_failures)\n    print(f'FALLO TOTAL: {max_retries} intentos agotados. Fallos consecutivos: {consecutive_failures}')\n    return None\n\ndef _handle_failure(attempt, max_retries, consecutive_failures, error_type):\n    if attempt == max_retries - 1:  # \u00daltimo intento\n        print(f'\u00daLTIMO INTENTO FALLADO - Tipo: {error_type}')\n        print(f'Fallos consecutivos acumulados: {consecutive_failures}')\n    else:\n        next_delay = 1 * (2 ** attempt)  # Pr\u00f3ximo delay exponencial\n        print(f'Preparando reintento con backoff exponencial de {next_delay}s')\n\ndef _activate_circuit_breaker(consecutive_failures):\n    if consecutive_failures >= 10:\n        circuit_open_time = 300\n        print('CIRCUIT BREAKER CR\u00cdTICO: 10+ fallos consecutivos - Pausando 5 minutos')\n    elif consecutive_failures >= 5:\n        circuit_open_time = 60\n        print('CIRCUIT BREAKER ACTIVADO: 5+ fallos consecutivos - Pausando 1 minuto')\n    else:\n        return  \n    \n    _fetch_qb_data._circuit_open_until = time.time() + circuit_open_time\n    _fetch_qb_data._consecutive_failures = consecutive_failures\n\n@data_loader\ndef load_data(*args, **kwargs):\n    \"\"\"\n    Template code for loading data from any source.\n    \n    Args:\n        fecha_inicio (str): Fecha de inicio en formato YYYY-MM-DD (requerido para backfill)\n        fecha_fin (str): Fecha de fin en formato YYYY-MM-DD (requerido para backfill)\n        chunk_days (int): N\u00famero de d\u00edas por chunk (opcional, default: 7)\n\n    Returns:\n        pandas.DataFrame: DataFrame con una fila por item\n    \"\"\"\n    realm_id = get_secret_value('qb_realm_id')\n    access_token = get_secret_value('qb_access_token')\n    minor_version = 75\n    base_url = 'https://sandbox-quickbooks.api.intuit.com'\n    \n    print(\"REFRESCANDO TOKEN\")\n    new_access_token, new_refresh_token = _refrescar_access_token()\n    \n    if new_access_token:\n        access_token = new_access_token  \n        print(f\"Token refrescado con exito\")\n    else:\n        print(\"Error al regrescar el token, puede estar expirado\")\n    \n    start_date_str = kwargs.get('fecha_inicio')\n    end_date_str = kwargs.get('fecha_fin')\n    chunk_days = kwargs.get('chunk_days', 7) \n    \n    # variables de recuperacion\n    resume_mode = kwargs.get('resume_mode', False)  # True para reanudar desde \u00faltimo exitoso\n    retry_failed_chunks = kwargs.get('retry_failed_chunks', False)  # True para reintentar fallos\n    verify_only = kwargs.get('verify_only', False)  # True para solo verificar sin procesar\n    skip_chunks = kwargs.get('skip_chunks', [])  # Lista de n\u00fameros de chunk a omitir\n    force_chunks = kwargs.get('force_chunks', [])  # Lista de n\u00fameros de chunk a forzar reproceso\n    \n    print(f\"CONFIGURACI\u00d3N DE PROCESAMIENTO\")\n    print(f\"Resume mode: {'ACTIVADO' if resume_mode else 'DESACTIVADO'}\")\n    print(f\"Retry failed chunks: {'ACTIVADO' if retry_failed_chunks else 'DESACTIVADO'}\")\n    print(f\"Verify only: {'Solo verificaci\u00f3n' if verify_only else 'Procesamiento normal'}\")\n    print(f\"Skip chunks: {skip_chunks if skip_chunks else 'Ninguno'}\")\n    print(f\"Force chunks: {force_chunks if force_chunks else 'Ninguno'}\")\n    \n    if not start_date_str or not end_date_str:\n        raise ValueError(\"Se requieren los par\u00e1metros 'fecha_inicio' y 'fecha_fin' en formato YYYY-MM-DD\")\n    \n    try:\n        start_date = datetime.strptime(start_date_str, '%Y-%m-%d').date()\n        end_date = datetime.strptime(end_date_str, '%Y-%m-%d').date()\n    except ValueError as e:\n        raise ValueError(f\"Formato de fecha inv\u00e1lido. Use YYYY-MM-DD: {e}\")\n    \n    if start_date > end_date:\n        raise ValueError(\"La fecha de inicio debe ser menor o igual a la fecha de fin\")\n    \n    # timestamp de ingesta\n    ingested_at_utc = kwargs.get('execution_date', datetime.utcnow())\n    \n    # Convertir a string si es datetime\n    if isinstance(ingested_at_utc, datetime):\n        ingested_at_utc_str = ingested_at_utc.isoformat() + 'Z'\n    else:\n        ingested_at_utc_str = str(ingested_at_utc)\n    \n    print(f'INICIO DEL BACKFILL DE QB ITEMS')\n    print(f'Rango completo: {start_date_str} a {end_date_str}')\n    print(f'Chunk size: {chunk_days} d\u00edas')\n    print(f'Ingested at: {ingested_at_utc_str}')\n    \n    chunks = []\n    current_date = start_date\n    chunk_number = 1\n    \n    while current_date <= end_date:\n        chunk_end = min(current_date + timedelta(days=chunk_days - 1), end_date)\n        chunks.append({\n            'chunk_number': chunk_number,\n            'start_date': current_date,\n            'end_date': chunk_end,\n            'start_date_str': current_date.strftime('%Y-%m-%d'),\n            'end_date_str': chunk_end.strftime('%Y-%m-%d')\n        })\n        current_date = chunk_end + timedelta(days=1)\n        chunk_number += 1\n    \n    print(f'Total de chunks a procesar: {len(chunks)}')\n    \n    # tracking de progreso y recuperacion\n    progress_tracker = {\n        'run_id': f\"{start_date_str}_{end_date_str}_{chunk_days}d_{ingested_at_utc_str.split('T')[0]}\",\n        'total_chunks': len(chunks),\n        'completed_chunks': [],\n        'failed_chunks': [],\n        'skipped_chunks': skip_chunks,\n        'processing_start': datetime.utcnow().isoformat() + 'Z'\n    }\n    \n    # resume/retry\n    chunks_to_process = []\n    for chunk in chunks:\n        chunk_num = chunk['chunk_number']\n        \n        # Verificar si saltar este chunk\n        if chunk_num in skip_chunks:\n            print(f\"Saltando chunk {chunk_num} (en skip_chunks)\")\n            progress_tracker['skipped_chunks'].append(chunk_num)\n            continue\n            \n        # Verificar si forzar reproceso\n        if force_chunks and chunk_num in force_chunks:\n            print(f\"Forzando reproceso del chunk {chunk_num}\")\n            chunks_to_process.append(chunk)\n            continue\n        \n        # modo verify_only, solo mostrar qu\u00e9 se har\u00eda\n        if verify_only:\n            print(f\"[VERIFY] Chunk {chunk_num}: {chunk['start_date_str']} a {chunk['end_date_str']}\")\n            continue\n            \n        if resume_mode:\n            print(f\"[RESUME] Verificando chunk {chunk_num}...\")\n        \n        chunks_to_process.append(chunk)\n    \n    if verify_only:\n        print(f\"\\nVERIFICACI\u00d3N COMPLETADA\")\n        print(f\"Total chunks definidos: {len(chunks)}\")\n        print(f\"Chunks a saltar: {len(skip_chunks)}\")\n        print(f\"Chunks a forzar: {len(force_chunks)}\")\n        print(f\"Chunks que se procesar\u00edan: {len(chunks_to_process)}\")\n        return pd.DataFrame()  # DataFrame vac\u00edo en modo verificaci\u00f3n\n    \n    print(f\"RESUMEN DE PROCESAMIENTO:\")\n    print(f\"  Total chunks definidos: {len(chunks)}\")\n    print(f\"  Chunks a procesar: {len(chunks_to_process)}\")\n    print(f\"  Chunks a saltar: {len(skip_chunks)}\")\n    print(f\"  Chunks forzados: {len(force_chunks) if force_chunks else 0}\")\n    \n    # Lista para almacenar todas las filas del DataFrame\n    all_rows = []\n    total_invoices = 0\n    total_pages = 0\n    processed_chunks_count = 0\n    \n    for chunk in chunks_to_process:\n        chunk_start_time = time.time()\n        processed_chunks_count += 1\n        \n        print(f'\\nPROCESANDO CHUNK {chunk[\"chunk_number\"]}/{len(chunks)} ({processed_chunks_count}/{len(chunks_to_process)} a procesar)')\n        print(f'Fechas procesadas: {chunk[\"start_date_str\"]} a {chunk[\"end_date_str\"]}')\n        \n        try:\n            # query para el chunk actual\n            start_utc = f\"{chunk['start_date_str']}T00:00:00Z\"\n            end_utc = f\"{chunk['end_date_str']}T23:59:59Z\"\n            \n            query = f\"select * from Item where MetaData.LastUpdatedTime >= '{start_utc}' and MetaData.LastUpdatedTime <= '{end_utc}'\"\n            \n            # p\u00e1ginas para este chunk\n            chunk_invoices = 0\n            chunk_pages = 0\n            max_results = 100\n            start_position = 1\n            page_number = 1\n            \n            while True:\n                page_start_time = time.time()\n                \n                # p\u00e1gina actual\n                data = _fetch_qb_data(\n                    realm_id=realm_id,\n                    access_token=access_token,\n                    query=query,\n                    base_url=base_url,\n                    minor_version=minor_version,\n                    start_position=start_position,\n                    max_results=max_results\n                )\n                \n                if not data or 'QueryResponse' not in data:\n                    print(f'  No se encontraron m\u00e1s datos en p\u00e1gina {page_number}')\n                    break\n                    \n                query_response = data['QueryResponse']\n                \n                if 'Item' not in query_response:\n                    print(f'  No se encontraron items en p\u00e1gina {page_number}')\n                    break\n                    \n                items = query_response['Item']\n                page_end_time = time.time()\n                page_duration = page_end_time - page_start_time\n                \n                # URL completa de la llamada API\n                paginated_query = f\"{query} STARTPOSITION {start_position} MAXRESULTS {max_results}\"\n                full_api_url = f\"{base_url.rstrip('/')}/v3/company/{realm_id}/query?query={paginated_query}&minorversion={minor_version}\"\n                \n                # Metadatos comunes de la p\u00e1gina\n                page_common_data = {\n                    'ingested_at_utc': ingested_at_utc_str,\n                    'extract_window_start_utc': start_utc,\n                    'extract_window_end_utc': end_utc,\n                    'page_number': page_number,\n                    'page_size': len(items),\n                    'request_payload': json.dumps({\n                        'full_api_url': full_api_url,\n                        'method': 'GET',\n                        'headers': {\n                            'Authorization': 'Bearer [HIDDEN]',\n                            'Accept': 'application/json',\n                            'Content-Type': 'text/plain'\n                        },\n                        'query_parameters': {\n                            'query': paginated_query,\n                            'minorversion': minor_version\n                        },\n                        'base_url': base_url,\n                        'realm_id': realm_id,\n                        'original_query': query\n                    })\n                }\n                \n                # Procesar cada item en la p\u00e1gina y agregar al DataFrame\n                for item in items:\n                    row = {\n                        'id': item.get('Id'),\n                        'payload': json.dumps(item),\n                        **page_common_data\n                    }\n                    all_rows.append(row)\n                \n                chunk_invoices += len(items)\n                chunk_pages += 1\n                \n                print(f'  P\u00e1gina {page_number}: {len(items)} items en {page_duration:.2f}s')\n                \n                # si recibimos menos registros de los solicitados, es la \u00faltima p\u00e1gina\n                if len(items) < max_results:\n                    break\n                    \n                # avanzar a la siguiente p\u00e1gina\n                start_position += max_results\n                page_number += 1\n            \n            chunk_end_time = time.time()\n            chunk_duration = chunk_end_time - chunk_start_time\n            \n            # Actualizar totales\n            total_invoices += chunk_invoices\n            total_pages += chunk_pages\n            \n            # Marcar chunk como completado exitosamente\n            progress_tracker['completed_chunks'].append(chunk['chunk_number'])\n            \n            # LOGS DEL TRAMO COMPLETADO\n            print(f'\\nCHUNK {chunk[\"chunk_number\"]} COMPLETADO')\n            print(f'Fechas procesadas: {chunk[\"start_date_str\"]} a {chunk[\"end_date_str\"]}')\n            print(f'P\u00e1ginas le\u00eddas: {chunk_pages}')\n            print(f'Filas insertadas: {chunk_invoices}')\n            print(f'Duraci\u00f3n total del chunk: {chunk_duration:.2f} segundos')\n            print(f'Promedio por p\u00e1gina: {chunk_duration/max(chunk_pages, 1):.2f} segundos')\n            print(f'Velocidad de ingesta: {chunk_invoices/max(chunk_duration, 0.1):.2f} items/segundo')\n            print(f'Progreso general: {chunk[\"chunk_number\"]}/{len(chunks)} chunks ({(chunk[\"chunk_number\"]/len(chunks)*100):.1f}%)')\n            print(f'Total acumulado hasta ahora: {total_invoices} items en {total_pages} p\u00e1ginas')\n            print('-' * 60)\n        \n        except Exception as chunk_error:\n            # Manejo de errores de chunk completo\n            chunk_end_time = time.time()\n            chunk_duration = chunk_end_time - chunk_start_time\n            \n            print(f'\\nERROR EN CHUNK {chunk[\"chunk_number\"]}')\n            print(f'Fechas afectadas: {chunk[\"start_date_str\"]} a {chunk[\"end_date_str\"]}')\n            print(f'Error: {str(chunk_error)}')\n            print(f'Duraci\u00f3n antes del error: {chunk_duration:.2f} segundos')\n            \n            # Marcar chunk como fallido\n            progress_tracker['failed_chunks'].append({\n                'chunk_number': chunk['chunk_number'],\n                'date_range': f\"{chunk['start_date_str']} a {chunk['end_date_str']}\",\n                'error': str(chunk_error),\n                'duration': chunk_duration\n            })\n            \n            # Decidir si continuar o fallar completamente\n            if retry_failed_chunks:\n                print(f\"Chunk fallido marcado para reintento posterior\")\n            else:\n                print(f\"Continuando con siguiente chunk (chunk fallido omitido)\")\n            \n            # Continuar con el siguiente chunk\n            continue\n    \n    print(f'\\nBACKFILL COMPLETADO')\n    print(f'Total chunks procesados: {len(chunks)}')\n    print(f'Total p\u00e1ginas: {total_pages}')\n    print(f'Rango procesado: {start_date_str} a {end_date_str}')\n    \n    # Crear DataFrame final\n    df = pd.DataFrame(all_rows)\n    \n    # eliminar duplicados:  esto puede ocurrir cuando items caen en dos rangos de fecha de consultas\n    if not df.empty:\n        filas_antes = len(df)\n        df = df.drop_duplicates(subset=['id'], keep='first')  # Mantener la primera ocurrencia\n        filas_despues = len(df)\n        duplicados_eliminados = filas_antes - filas_despues\n    \n    # reordenar columnas en el orden deseado \n    if not df.empty:\n        column_order = [\n            'id',\n            'payload', \n            'ingested_at_utc',\n            'extract_window_start_utc',\n            'extract_window_end_utc',\n            'page_number',\n            'page_size',\n            'request_payload'\n        ]\n\n        available_columns = [col for col in column_order if col in df.columns]\n        df = df[available_columns]\n\n    print(f'Total items(luego de eliminar duplicados): {len(df)}')\n    print(f\"\\nDataFrame creado con {len(df)} items\")\n    \n    return df\n\n\n@test\ndef test_output(output, *args) -> None:\n    \"\"\"\n    Template code for testing the output of the block.\n    \"\"\"\n    assert output is not None, 'The output is undefined'\n", "file_path": "data_loaders/ingest_qb_items.py", "language": "python", "type": "data_loader", "uuid": "ingest_qb_items"}, "data_loaders/load_titanic.py:data_loader:python:load titanic": {"content": "import io\nimport pandas as pd\nimport requests\nfrom pandas import DataFrame\n\nif 'data_loader' not in globals():\n    from mage_ai.data_preparation.decorators import data_loader\nif 'test' not in globals():\n    from mage_ai.data_preparation.decorators import test\n\n\n@data_loader\ndef load_data_from_api(**kwargs) -> DataFrame:\n    \"\"\"\n    Template for loading data from API\n    \"\"\"\n    url = 'https://raw.githubusercontent.com/datasciencedojo/datasets/master/titanic.csv?raw=True'\n\n    return pd.read_csv(url)\n\n\n@test\ndef test_output(df) -> None:\n    \"\"\"\n    Template code for testing the output of the block.\n    \"\"\"\n    assert df is not None, 'The output is undefined'\n", "file_path": "data_loaders/load_titanic.py", "language": "python", "type": "data_loader", "uuid": "load_titanic"}, "data_loaders/marvelous_wizard.py:data_loader:python:marvelous wizard": {"content": "import io\nimport pandas as pd\nimport requests\nif 'data_loader' not in globals():\n    from mage_ai.data_preparation.decorators import data_loader\nif 'test' not in globals():\n    from mage_ai.data_preparation.decorators import test\n\n\n@data_loader\ndef load_data_from_api(*args, **kwargs):\n    \"\"\"\n    Template for loading data from API\n    \"\"\"\n    url = ''\n    response = requests.get(url)\n\n    return pd.read_csv(io.StringIO(response.text), sep=',')\n\n\n@test\ndef test_output(output, *args) -> None:\n    \"\"\"\n    Template code for testing the output of the block.\n    \"\"\"\n    assert output is not None, 'The output is undefined'\n", "file_path": "data_loaders/marvelous_wizard.py", "language": "python", "type": "data_loader", "uuid": "marvelous_wizard"}, "data_loaders/mystical_ancient.py:data_loader:python:mystical ancient": {"content": "import io\nimport pandas as pd\nimport requests\nif 'data_loader' not in globals():\n    from mage_ai.data_preparation.decorators import data_loader\nif 'test' not in globals():\n    from mage_ai.data_preparation.decorators import test\n\n\n@data_loader\ndef load_data_from_api(*args, **kwargs):\n    \"\"\"\n    Template for loading data from API\n    \"\"\"\n    url = ''\n    response = requests.get(url)\n\n    return pd.read_csv(io.StringIO(response.text), sep=',')\n\n\n@test\ndef test_output(output, *args) -> None:\n    \"\"\"\n    Template code for testing the output of the block.\n    \"\"\"\n    assert output is not None, 'The output is undefined'\n", "file_path": "data_loaders/mystical_ancient.py", "language": "python", "type": "data_loader", "uuid": "mystical_ancient"}, "data_loaders/url_ingest_ny_taxi.py:data_loader:python:url ingest ny taxi": {"content": "import pandas as pd\nimport os\n\nif 'data_loader' not in globals():\n    from mage_ai.data_preparation.decorators import data_loader\nif 'test' not in globals():\n    from mage_ai.data_preparation.decorators import test\n\n\n@data_loader\ndef load_data(*args, **kwargs):\n    \"\"\"\n    Template code for loading data from any source.\n\n    Returns:\n        Anything (e.g. data frame, dictionary, array, int, str, etc.)\n    \"\"\"\n    # Specify your data loading logic here\n    URL = 'https://github.com/DataTalksClub/nyc-tlc-data/releases/download/yellow/yellow_tripdata_2021-01.csv.gz'\n    output_name= 'raw_data.csv.gz'\n    os.system(f'wget {URL} -O {output_name}')\n \n    data_it = pd.read_csv(output_name, iterator=True,chunksize=100000)\n\n    final_data=pd.DataFrame()\n\n    i =   0\n\n    while True:\n\n        try:\n            data_raw=next(data_it)\n            if i==0:\n                final_data = data_raw\n            else: \n                pd.concat([final_data,data_raw], axis=0)\n            i+=1\n\n        except StopIteration:\n            break  \n\n    return  final_data\n\n\n@test\ndef test_output(output, *args) -> None:\n    \"\"\"\n    Template code for testing the output of the block.\n    \"\"\"\n    assert output is not None, 'The output is undefined'\n", "file_path": "data_loaders/url_ingest_ny_taxi.py", "language": "python", "type": "data_loader", "uuid": "url_ingest_ny_taxi"}, "transformers/crimson_dew.py:transformer:python:crimson dew": {"content": "from mage_ai.data_cleaner.transformer_actions.base import BaseAction\nfrom mage_ai.data_cleaner.transformer_actions.constants import ActionType, Axis\nfrom mage_ai.data_cleaner.transformer_actions.utils import build_transformer_action\nfrom pandas import DataFrame\n\nif 'transformer' not in globals():\n    from mage_ai.data_preparation.decorators import transformer\nif 'test' not in globals():\n    from mage_ai.data_preparation.decorators import test\n\n\n@transformer\ndef execute_transformer_action(df: DataFrame, *args, **kwargs) -> DataFrame:\n    \"\"\"\n    Execute Transformer Action: ActionType.DIFF\n\n    Calculates difference from previous row along column.\n\n    Docs: https://docs.mage.ai/guides/transformer-blocks#difference\n    \"\"\"\n    action = build_transformer_action(\n        df,\n        action_type=ActionType.DIFF,\n        arguments=[],  # Specify at most one column to compute difference with\n        axis=Axis.COLUMN,\n        outputs=[{'uuid': 'new_diff_column', 'column_type': 'number_with_decimals'}],\n    )\n\n    return BaseAction(action).execute(df)\n\n\n@test\ndef test_output(output, *args) -> None:\n    \"\"\"\n    Template code for testing the output of the block.\n    \"\"\"\n    assert output is not None, 'The output is undefined'\n", "file_path": "transformers/crimson_dew.py", "language": "python", "type": "transformer", "uuid": "crimson_dew"}, "transformers/fill_in_missing_values.py:transformer:python:fill in missing values": {"content": "from pandas import DataFrame\nimport math\n\nif 'transformer' not in globals():\n    from mage_ai.data_preparation.decorators import transformer\nif 'test' not in globals():\n    from mage_ai.data_preparation.decorators import test\n\ndef select_number_columns(df: DataFrame) -> DataFrame:\n    return df[['Age', 'Fare', 'Parch', 'Pclass', 'SibSp', 'Survived']]\n\n\ndef fill_missing_values_with_median(df: DataFrame) -> DataFrame:\n    for col in df.columns:\n        values = sorted(df[col].dropna().tolist())\n        median_value = values[math.floor(len(values) / 2)]\n        df[[col]] = df[[col]].fillna(median_value)\n    return df\n\n\n@transformer\ndef transform_df(df: DataFrame, *args, **kwargs) -> DataFrame:\n    \"\"\"\n    Template code for a transformer block.\n\n    Add more parameters to this function if this block has multiple parent blocks.\n    There should be one parameter for each output variable from each parent block.\n\n    Args:\n        df (DataFrame): Data frame from parent block.\n\n    Returns:\n        DataFrame: Transformed data frame\n    \"\"\"\n    # Specify your transformation logic here\n\n    return fill_missing_values_with_median(select_number_columns(df))\n\n\n@test\ndef test_output(df) -> None:\n    \"\"\"\n    Template code for testing the output of the block.\n    \"\"\"\n    assert df is not None, 'The output is undefined'\n", "file_path": "transformers/fill_in_missing_values.py", "language": "python", "type": "transformer", "uuid": "fill_in_missing_values"}, "transformers/ny_taxi_clean.py:transformer:python:ny taxi clean": {"content": "from pandas import DataFrame, to_datetime\n\nif 'transformer' not in globals():\n    from mage_ai.data_preparation.decorators import transformer\nif 'test' not in globals():\n    from mage_ai.data_preparation.decorators import test\n\n\n@transformer\ndef execute_transformer_action(df: DataFrame, *args, **kwargs) -> DataFrame:\n    \"\"\"\n    Execute Transformer Action: ActionType.FIX_SYNTAX_ERRORS\n\n    This marks any improperly formatted values in each column specified\n    as invalid.\n\n    Docs: https://docs.mage.ai/guides/transformer-blocks#fix-syntax-errors\n    \"\"\"\n    df.tpep_pickup_datetime = to_datetime(df.tpep_pickup_datetime)\n    df.tpep_dropoff_datetime = to_datetime(df.tpep_dropoff_datetime)\n\n    return df\n\n\n@test\ndef test_output(output, *args) -> None:\n    \"\"\"\n    Template code for testing the output of the block.\n    \"\"\"\n    assert output is not None, 'The output is undefined'\n", "file_path": "transformers/ny_taxi_clean.py", "language": "python", "type": "transformer", "uuid": "ny_taxi_clean"}, "transformers/transform_qb_invoices.py:transformer:python:transform qb invoices": {"content": "import pandas as pd\nimport json\n\nif 'transformer' not in globals():\n    from mage_ai.data_preparation.decorators import transformer\nif 'test' not in globals():\n    from mage_ai.data_preparation.decorators import test\n\n\n@transformer\ndef transform(data, *args, **kwargs):\n    \"\"\"\n    Transforma all_pages_metadata a un DataFrame donde cada fila es un invoice individual.\n\n    Args:\n        data: all_pages_metadata del script de ingesta\n\n    Returns:\n        pandas.DataFrame: DataFrame con una fila por invoice\n    \"\"\"\n    # Lista para almacenar todas las filas del DataFrame\n    rows = []\n    \n    # Procesar cada p\u00e1gina\n    for page_metadata in data:\n        # Extraer metadatos comunes de la p\u00e1gina\n        page_common_data = {\n            'ingested_at_utc': page_metadata.get('ingested_at_utc'),\n            'extract_window_start_utc': page_metadata.get('extract_window_start_utc'),\n            'extract_window_end_utc': page_metadata.get('extract_window_end_utc'),\n            'page_number': page_metadata.get('page_number'),\n            'page_size': page_metadata.get('page_size'),\n            'request_payload': json.dumps(page_metadata.get('request_payload', {}))\n        }\n        \n        # Procesar cada invoice en la p\u00e1gina\n        invoices = page_metadata.get('invoices', [])\n        \n        for invoice in invoices:\n            # Crear fila para este invoice\n            row = {\n                'id': invoice.get('Id'),  # ID del invoice\n                'payload': json.dumps(invoice),  # Payload completo del invoice como JSON\n                **page_common_data  # Agregar metadatos comunes de la p\u00e1gina\n            }\n            \n            rows.append(row)\n    \n    # Crear DataFrame\n    df = pd.DataFrame(rows)\n    \n    # Reordenar columnas en el orden deseado\n    column_order = [\n        'id',\n        'payload', \n        'ingested_at_utc',\n        'extract_window_start_utc',\n        'extract_window_end_utc',\n        'page_number',\n        'page_size',\n        'request_payload'\n    ]\n    \n    df = df[column_order]\n    \n    print(f\"DataFrame creado con {len(df)} invoices de {len(data)} p\u00e1ginas\")\n    \n    return df\n\n\n@test\ndef test_output(output, *args) -> None:\n    \"\"\"\n    Template code for testing the output of the block.\n    \"\"\"\n    assert output is not None, 'The output is undefined'\n", "file_path": "transformers/transform_qb_invoices.py", "language": "python", "type": "transformer", "uuid": "transform_qb_invoices"}, "pipelines/qb_customers_backfill/metadata.yaml:pipeline:yaml:qb customers backfill/metadata": {"content": "blocks:\n- all_upstream_blocks_executed: true\n  color: null\n  configuration: {}\n  downstream_blocks:\n  - export_qb_customers\n  executor_config: null\n  executor_type: local_python\n  has_callback: false\n  language: python\n  name: ingest_qb_customers\n  retry_config: null\n  status: executed\n  timeout: null\n  type: data_loader\n  upstream_blocks: []\n  uuid: ingest_qb_customers\n- all_upstream_blocks_executed: true\n  color: null\n  configuration: {}\n  downstream_blocks: []\n  executor_config: null\n  executor_type: local_python\n  has_callback: false\n  language: python\n  name: export_qb_customers\n  retry_config: null\n  status: executed\n  timeout: null\n  type: data_exporter\n  upstream_blocks:\n  - ingest_qb_customers\n  uuid: export_qb_customers\ncache_block_output_in_memory: false\ncallbacks: []\nconcurrency_config: {}\nconditionals: []\ncreated_at: '2025-09-12 22:07:10.639926+00:00'\ndata_integration: null\ndescription: null\nexecutor_config: {}\nexecutor_count: 1\nexecutor_type: null\nextensions: {}\nname: qb_customers_backfill\nnotification_config: {}\nremote_variables_dir: null\nretry_config: {}\nrun_pipeline_in_one_process: false\nsettings:\n  triggers: null\nspark_config: {}\ntags: []\ntype: python\nuuid: qb_customers_backfill\nvariables:\n  fecha_fin: '2026-01-01'\n  fecha_inicio: '2025-05-01'\nvariables_dir: /home/src/mage_data/scheduler\nwidgets: []\n", "file_path": "pipelines/qb_customers_backfill/metadata.yaml", "language": "yaml", "type": "pipeline", "uuid": "qb_customers_backfill/metadata"}, "pipelines/qb_customers_backfill/__init__.py:pipeline:python:qb customers backfill/  init  ": {"content": "", "file_path": "pipelines/qb_customers_backfill/__init__.py", "language": "python", "type": "pipeline", "uuid": "qb_customers_backfill/__init__"}, "pipelines/qb_invoices_backfill/metadata.yaml:pipeline:yaml:qb invoices backfill/metadata": {"content": "blocks:\n- all_upstream_blocks_executed: true\n  color: null\n  configuration: {}\n  downstream_blocks:\n  - export_qb_invoices\n  executor_config: null\n  executor_type: local_python\n  has_callback: false\n  language: python\n  name: ingest_qb_invoices\n  retry_config: null\n  status: executed\n  timeout: null\n  type: data_loader\n  upstream_blocks: []\n  uuid: ingest_qb_invoices\n- all_upstream_blocks_executed: true\n  color: null\n  configuration: {}\n  downstream_blocks: []\n  executor_config: null\n  executor_type: local_python\n  has_callback: false\n  language: python\n  name: export_qb_invoices\n  retry_config: null\n  status: executed\n  timeout: null\n  type: data_exporter\n  upstream_blocks:\n  - ingest_qb_invoices\n  uuid: export_qb_invoices\ncache_block_output_in_memory: false\ncallbacks: []\nconcurrency_config: {}\nconditionals: []\ncreated_at: '2025-09-04 18:58:07.373084+00:00'\ndata_integration: null\ndescription: null\nexecutor_config: {}\nexecutor_count: 1\nexecutor_type: null\nextensions: {}\nname: qb_invoices_backfill\nnotification_config: {}\nremote_variables_dir: null\nretry_config: {}\nrun_pipeline_in_one_process: false\nsettings:\n  triggers: null\nspark_config: {}\ntags: []\ntype: python\nuuid: qb_invoices_backfill\nvariables:\n  fecha_fin: '2025-05-01'\n  fecha_inicio: '2025-04-01'\nvariables_dir: /home/src/mage_data/scheduler\nwidgets: []\n", "file_path": "pipelines/qb_invoices_backfill/metadata.yaml", "language": "yaml", "type": "pipeline", "uuid": "qb_invoices_backfill/metadata"}, "pipelines/qb_invoices_backfill/triggers.yaml:pipeline:yaml:qb invoices backfill/triggers": {"content": "triggers: []\n", "file_path": "pipelines/qb_invoices_backfill/triggers.yaml", "language": "yaml", "type": "pipeline", "uuid": "qb_invoices_backfill/triggers"}, "pipelines/qb_invoices_backfill/__init__.py:pipeline:python:qb invoices backfill/  init  ": {"content": "", "file_path": "pipelines/qb_invoices_backfill/__init__.py", "language": "python", "type": "pipeline", "uuid": "qb_invoices_backfill/__init__"}, "pipelines/qb_items_backfill/metadata.yaml:pipeline:yaml:qb items backfill/metadata": {"content": "blocks:\n- all_upstream_blocks_executed: true\n  color: null\n  configuration: {}\n  downstream_blocks:\n  - export_qb_items\n  executor_config: null\n  executor_type: local_python\n  has_callback: false\n  language: python\n  name: ingest_qb_items\n  retry_config: null\n  status: executed\n  timeout: null\n  type: data_loader\n  upstream_blocks: []\n  uuid: ingest_qb_items\n- all_upstream_blocks_executed: true\n  color: null\n  configuration: {}\n  downstream_blocks: []\n  executor_config: null\n  executor_type: local_python\n  has_callback: false\n  language: python\n  name: export_qb_items\n  retry_config: null\n  status: executed\n  timeout: null\n  type: data_exporter\n  upstream_blocks:\n  - ingest_qb_items\n  uuid: export_qb_items\ncache_block_output_in_memory: false\ncallbacks: []\nconcurrency_config: {}\nconditionals: []\ncreated_at: '2025-09-12 23:13:04.368313+00:00'\ndata_integration: null\ndescription: null\nexecutor_config: {}\nexecutor_count: 1\nexecutor_type: null\nextensions: {}\nname: qb_items_backfill\nnotification_config: {}\nremote_variables_dir: null\nretry_config: {}\nrun_pipeline_in_one_process: false\nsettings:\n  triggers: null\nspark_config: {}\ntags: []\ntype: python\nuuid: qb_items_backfill\nvariables:\n  fecha_fin: '2026-01-01'\n  fecha_inicio: '2025-01-01'\nvariables_dir: /home/src/mage_data/scheduler\nwidgets: []\n", "file_path": "pipelines/qb_items_backfill/metadata.yaml", "language": "yaml", "type": "pipeline", "uuid": "qb_items_backfill/metadata"}, "pipelines/qb_items_backfill/__init__.py:pipeline:python:qb items backfill/  init  ": {"content": "", "file_path": "pipelines/qb_items_backfill/__init__.py", "language": "python", "type": "pipeline", "uuid": "qb_items_backfill/__init__"}}, "custom_block_template": {}, "mage_template": {"data_loaders/airtable.py:data_loader:python:Airtable:Load a Table from Airtable App.": {"block_type": "data_loader", "description": "Load a Table from Airtable App.", "language": "python", "name": "Airtable", "path": "data_loaders/airtable.py"}, "data_loaders/deltalake/s3.py:data_loader:python:Amazon S3:Load a Delta Table from Amazon S3.:Delta Lake": {"block_type": "data_loader", "description": "Load a Delta Table from Amazon S3.", "groups": ["Delta Lake"], "language": "python", "name": "Amazon S3", "path": "data_loaders/deltalake/s3.py"}, "data_loaders/deltalake/azure_blob_storage.py:data_loader:python:Azure Blob Storage:Load a Delta Table from Azure Blob Storage.:Delta Lake": {"block_type": "data_loader", "description": "Load a Delta Table from Azure Blob Storage.", "groups": ["Delta Lake"], "language": "python", "name": "Azure Blob Storage", "path": "data_loaders/deltalake/azure_blob_storage.py"}, "data_loaders/deltalake/gcs.py:data_loader:python:Google Cloud Storage:Load a Delta Table from Google Cloud Storage.:Delta Lake": {"block_type": "data_loader", "description": "Load a Delta Table from Google Cloud Storage.", "groups": ["Delta Lake"], "language": "python", "name": "Google Cloud Storage", "path": "data_loaders/deltalake/gcs.py"}, "data_loaders/mongodb.py:data_loader:python:MongoDB:Load data from MongoDB.:Databases (NoSQL)": {"block_type": "data_loader", "description": "Load data from MongoDB.", "groups": ["Databases (NoSQL)"], "language": "python", "name": "MongoDB", "path": "data_loaders/mongodb.py"}, "data_loaders/mssql.py:data_loader:python:MSSQL:Load data from MSSQL.:Databases": {"block_type": "data_loader", "description": "Load data from MSSQL.", "groups": ["Databases"], "language": "python", "name": "MSSQL", "path": "data_loaders/mssql.py"}, "data_exporters/deltalake/s3.py:data_exporter:python:Amazon S3:Export data to a Delta Table in Amazon S3.:Delta Lake": {"block_type": "data_exporter", "description": "Export data to a Delta Table in Amazon S3.", "groups": ["Delta Lake"], "language": "python", "name": "Amazon S3", "path": "data_exporters/deltalake/s3.py"}, "data_exporters/deltalake/azure_blob_storage.py:data_exporter:python:Azure Blob Storage:Export data to a Delta Table in Azure Blob Storage.:Delta Lake": {"block_type": "data_exporter", "description": "Export data to a Delta Table in Azure Blob Storage.", "groups": ["Delta Lake"], "language": "python", "name": "Azure Blob Storage", "path": "data_exporters/deltalake/azure_blob_storage.py"}, "data_exporters/deltalake/gcs.py:data_exporter:python:Google Cloud Storage:Export data to a Delta Table in Google Cloud Storage.:Delta Lake": {"block_type": "data_exporter", "description": "Export data to a Delta Table in Google Cloud Storage.", "groups": ["Delta Lake"], "language": "python", "name": "Google Cloud Storage", "path": "data_exporters/deltalake/gcs.py"}, "data_exporters/mongodb.py:data_exporter:python:MongoDB:Export data to MongoDB.": {"block_type": "data_exporter", "description": "Export data to MongoDB.", "language": "python", "name": "MongoDB", "path": "data_exporters/mongodb.py"}, "data_exporters/mssql.py:data_exporter:python:MSSQL:Export data to MSSQL.:Databases": {"block_type": "data_exporter", "description": "Export data to MSSQL.", "groups": ["Databases"], "language": "python", "name": "MSSQL", "path": "data_exporters/mssql.py"}, "data_loaders/orchestration/triggers/default.jinja:data_loader:python:Trigger pipeline:Trigger another pipeline to run.:Orchestration": {"block_type": "data_loader", "description": "Trigger another pipeline to run.", "groups": ["Orchestration"], "language": "python", "name": "Trigger pipeline", "path": "data_loaders/orchestration/triggers/default.jinja"}, "data_exporters/orchestration/triggers/default.jinja:data_exporter:python:Trigger pipeline:Trigger another pipeline to run.:Orchestration": {"block_type": "data_exporter", "description": "Trigger another pipeline to run.", "groups": ["Orchestration"], "language": "python", "name": "Trigger pipeline", "path": "data_exporters/orchestration/triggers/default.jinja"}, "callbacks/base.jinja:callback:python:Base template:Base template with empty functions.": {"block_type": "callback", "description": "Base template with empty functions.", "language": "python", "name": "Base template", "path": "callbacks/base.jinja"}, "callbacks/orchestration/triggers/default.jinja:callback:python:Trigger pipeline:Trigger another pipeline to run.:Orchestration": {"block_type": "callback", "description": "Trigger another pipeline to run.", "groups": ["Orchestration"], "language": "python", "name": "Trigger pipeline", "path": "callbacks/orchestration/triggers/default.jinja"}, "conditionals/base.jinja:conditional:python:Base template:Base template with empty functions.": {"block_type": "conditional", "description": "Base template with empty functions.", "language": "python", "name": "Base template", "path": "conditionals/base.jinja"}, "data_loaders/default.jinja:data_loader:python:Base template (generic)": {"block_type": "data_loader", "language": "python", "name": "Base template (generic)", "path": "data_loaders/default.jinja"}, "data_loaders/s3.py:data_loader:python:Amazon S3:Data lakes": {"block_type": "data_loader", "groups": ["Data lakes"], "language": "python", "name": "Amazon S3", "path": "data_loaders/s3.py"}, "data_loaders/azure_blob_storage.py:data_loader:python:Azure Blob Storage:Data lakes": {"block_type": "data_loader", "groups": ["Data lakes"], "language": "python", "name": "Azure Blob Storage", "path": "data_loaders/azure_blob_storage.py"}, "data_loaders/google_cloud_storage.py:data_loader:python:Google Cloud Storage:Data lakes": {"block_type": "data_loader", "groups": ["Data lakes"], "language": "python", "name": "Google Cloud Storage", "path": "data_loaders/google_cloud_storage.py"}, "data_loaders/redshift.py:data_loader:python:Amazon Redshift:Data warehouses": {"block_type": "data_loader", "groups": ["Data warehouses"], "language": "python", "name": "Amazon Redshift", "path": "data_loaders/redshift.py"}, "data_loaders/bigquery.py:data_loader:python:Google BigQuery:Load data from Google BigQuery.:Data warehouses": {"block_type": "data_loader", "description": "Load data from Google BigQuery.", "groups": ["Data warehouses"], "language": "python", "name": "Google BigQuery", "path": "data_loaders/bigquery.py"}, "data_loaders/snowflake.py:data_loader:python:Snowflake:Data warehouses": {"block_type": "data_loader", "groups": ["Data warehouses"], "language": "python", "name": "Snowflake", "path": "data_loaders/snowflake.py"}, "data_loaders/algolia.py:data_loader:python:Algolia:Databases": {"block_type": "data_loader", "groups": ["Databases"], "language": "python", "name": "Algolia", "path": "data_loaders/algolia.py"}, "data_loaders/chroma.py:data_loader:python:Chroma:Databases": {"block_type": "data_loader", "groups": ["Databases"], "language": "python", "name": "Chroma", "path": "data_loaders/chroma.py"}, "data_loaders/duckdb.py:data_loader:python:DuckDB:Databases": {"block_type": "data_loader", "groups": ["Databases"], "language": "python", "name": "DuckDB", "path": "data_loaders/duckdb.py"}, "data_loaders/mysql.py:data_loader:python:MySQL:Databases": {"block_type": "data_loader", "groups": ["Databases"], "language": "python", "name": "MySQL", "path": "data_loaders/mysql.py"}, "data_loaders/oracledb.py:data_loader:python:Oracle DB:Databases": {"block_type": "data_loader", "groups": ["Databases"], "language": "python", "name": "Oracle DB", "path": "data_loaders/oracledb.py"}, "data_loaders/postgres.py:data_loader:python:PostgreSQL:Databases": {"block_type": "data_loader", "groups": ["Databases"], "language": "python", "name": "PostgreSQL", "path": "data_loaders/postgres.py"}, "data_loaders/qdrant.py:data_loader:python:Qdrant:Databases": {"block_type": "data_loader", "groups": ["Databases"], "language": "python", "name": "Qdrant", "path": "data_loaders/qdrant.py"}, "data_loaders/weaviate.py:data_loader:python:Weaviate:Databases": {"block_type": "data_loader", "groups": ["Databases"], "language": "python", "name": "Weaviate", "path": "data_loaders/weaviate.py"}, "data_loaders/api.py:data_loader:python:API:Fetch data from an API request.": {"block_type": "data_loader", "description": "Fetch data from an API request.", "language": "python", "name": "API", "path": "data_loaders/api.py"}, "data_loaders/file.py:data_loader:python:Local file:Load data from a file on your machine.": {"block_type": "data_loader", "description": "Load data from a file on your machine.", "language": "python", "name": "Local file", "path": "data_loaders/file.py"}, "data_loaders/google_sheets.py:data_loader:python:Google Sheets:Load data from a worksheet in Google Sheets.": {"block_type": "data_loader", "description": "Load data from a worksheet in Google Sheets.", "language": "python", "name": "Google Sheets", "path": "data_loaders/google_sheets.py"}, "data_loaders/druid.py:data_loader:python:Druid": {"block_type": "data_loader", "language": "python", "name": "Druid", "path": "data_loaders/druid.py"}, "transformers/default.jinja:transformer:python:Base template (generic)": {"block_type": "transformer", "language": "python", "name": "Base template (generic)", "path": "transformers/default.jinja"}, "transformers/data_warehouse_transformer.jinja:transformer:python:Amazon Redshift:Data warehouses": {"block_type": "transformer", "groups": ["Data warehouses"], "language": "python", "name": "Amazon Redshift", "path": "transformers/data_warehouse_transformer.jinja", "template_variables": {"additional_args": "\n        loader.commit() # Permanently apply database changes", "data_source": "redshift", "data_source_handler": "Redshift"}}, "transformers/data_warehouse_transformer.jinja:transformer:python:Google BigQuery:Data warehouses": {"block_type": "transformer", "groups": ["Data warehouses"], "language": "python", "name": "Google BigQuery", "path": "transformers/data_warehouse_transformer.jinja", "template_variables": {"additional_args": "", "data_source": "bigquery", "data_source_handler": "BigQuery"}}, "transformers/data_warehouse_transformer.jinja:transformer:python:Snowflake:Data warehouses": {"block_type": "transformer", "groups": ["Data warehouses"], "language": "python", "name": "Snowflake", "path": "transformers/data_warehouse_transformer.jinja", "template_variables": {"additional_args": "\n        loader.commit() # Permanently apply database changes", "data_source": "snowflake", "data_source_handler": "Snowflake"}}, "transformers/data_warehouse_transformer.jinja:transformer:python:PostgreSQL:Databases": {"block_type": "transformer", "groups": ["Databases"], "language": "python", "name": "PostgreSQL", "path": "transformers/data_warehouse_transformer.jinja", "template_variables": {"additional_args": "\n        loader.commit() # Permanently apply database changes", "data_source": "postgres", "data_source_handler": "Postgres"}}, "transformers/transformer_actions/row/drop_duplicate.py:transformer:python:Drop duplicate rows:Row actions": {"block_type": "transformer", "groups": ["Row actions"], "language": "python", "name": "Drop duplicate rows", "path": "transformers/transformer_actions/row/drop_duplicate.py"}, "transformers/transformer_actions/row/filter.py:transformer:python:Filter rows:Row actions": {"block_type": "transformer", "groups": ["Row actions"], "language": "python", "name": "Filter rows", "path": "transformers/transformer_actions/row/filter.py"}, "transformers/transformer_actions/row/remove.py:transformer:python:Remove rows:Row actions": {"block_type": "transformer", "groups": ["Row actions"], "language": "python", "name": "Remove rows", "path": "transformers/transformer_actions/row/remove.py"}, "transformers/transformer_actions/row/sort.py:transformer:python:Sort rows:Row actions": {"block_type": "transformer", "groups": ["Row actions"], "language": "python", "name": "Sort rows", "path": "transformers/transformer_actions/row/sort.py"}, "transformers/transformer_actions/column/average.py:transformer:python:Average value of column:Column actions:Aggregate": {"block_type": "transformer", "groups": ["Column actions", "Aggregate"], "language": "python", "name": "Average value of column", "path": "transformers/transformer_actions/column/average.py"}, "transformers/transformer_actions/column/count_distinct.py:transformer:python:Count unique values in column:Column actions:Aggregate": {"block_type": "transformer", "groups": ["Column actions", "Aggregate"], "language": "python", "name": "Count unique values in column", "path": "transformers/transformer_actions/column/count_distinct.py"}, "transformers/transformer_actions/column/first.py:transformer:python:First value in column:Column actions:Aggregate": {"block_type": "transformer", "groups": ["Column actions", "Aggregate"], "language": "python", "name": "First value in column", "path": "transformers/transformer_actions/column/first.py"}, "transformers/transformer_actions/column/last.py:transformer:python:Last value in column:Column actions:Aggregate": {"block_type": "transformer", "groups": ["Column actions", "Aggregate"], "language": "python", "name": "Last value in column", "path": "transformers/transformer_actions/column/last.py"}, "transformers/transformer_actions/column/max.py:transformer:python:Maximum value in column:Column actions:Aggregate": {"block_type": "transformer", "groups": ["Column actions", "Aggregate"], "language": "python", "name": "Maximum value in column", "path": "transformers/transformer_actions/column/max.py"}, "transformers/transformer_actions/column/median.py:transformer:python:Median value in column:Column actions:Aggregate": {"block_type": "transformer", "groups": ["Column actions", "Aggregate"], "language": "python", "name": "Median value in column", "path": "transformers/transformer_actions/column/median.py"}, "transformers/transformer_actions/column/min.py:transformer:python:Min value in column:Column actions:Aggregate": {"block_type": "transformer", "groups": ["Column actions", "Aggregate"], "language": "python", "name": "Min value in column", "path": "transformers/transformer_actions/column/min.py"}, "transformers/transformer_actions/column/sum.py:transformer:python:Sum of all values in column:Column actions:Aggregate": {"block_type": "transformer", "groups": ["Column actions", "Aggregate"], "language": "python", "name": "Sum of all values in column", "path": "transformers/transformer_actions/column/sum.py"}, "transformers/transformer_actions/column/count.py:transformer:python:Total count of values in column:Column actions:Aggregate": {"block_type": "transformer", "groups": ["Column actions", "Aggregate"], "language": "python", "name": "Total count of values in column", "path": "transformers/transformer_actions/column/count.py"}, "transformers/transformer_actions/column/clean_column_name.py:transformer:python:Clean column name:Column actions:Formatting": {"block_type": "transformer", "groups": ["Column actions", "Formatting"], "language": "python", "name": "Clean column name", "path": "transformers/transformer_actions/column/clean_column_name.py"}, "transformers/transformer_actions/column/fix_syntax_errors.py:transformer:python:Fix syntax errors:Column actions:Formatting": {"block_type": "transformer", "groups": ["Column actions", "Formatting"], "language": "python", "name": "Fix syntax errors", "path": "transformers/transformer_actions/column/fix_syntax_errors.py"}, "transformers/transformer_actions/column/reformat.py:transformer:python:Reformat values in column:Column actions:Formatting": {"block_type": "transformer", "groups": ["Column actions", "Formatting"], "language": "python", "name": "Reformat values in column", "path": "transformers/transformer_actions/column/reformat.py"}, "transformers/transformer_actions/column/select.py:transformer:python:Keep column(s):Column actions:Column removal": {"block_type": "transformer", "groups": ["Column actions", "Column removal"], "language": "python", "name": "Keep column(s)", "path": "transformers/transformer_actions/column/select.py"}, "transformers/transformer_actions/column/remove.py:transformer:python:Remove column(s):Column actions:Column removal": {"block_type": "transformer", "groups": ["Column actions", "Column removal"], "language": "python", "name": "Remove column(s)", "path": "transformers/transformer_actions/column/remove.py"}, "transformers/transformer_actions/column/shift_down.py:transformer:python:Shift row values down:Column actions:Shift": {"block_type": "transformer", "groups": ["Column actions", "Shift"], "language": "python", "name": "Shift row values down", "path": "transformers/transformer_actions/column/shift_down.py"}, "transformers/transformer_actions/column/shift_up.py:transformer:python:Shift row values up:Column actions:Shift": {"block_type": "transformer", "groups": ["Column actions", "Shift"], "language": "python", "name": "Shift row values up", "path": "transformers/transformer_actions/column/shift_up.py"}, "transformers/transformer_actions/column/normalize.py:transformer:python:Normalize data:Column actions:Feature scaling": {"block_type": "transformer", "groups": ["Column actions", "Feature scaling"], "language": "python", "name": "Normalize data", "path": "transformers/transformer_actions/column/normalize.py"}, "transformers/transformer_actions/column/standardize.py:transformer:python:Standardize data:Column actions:Feature scaling": {"block_type": "transformer", "groups": ["Column actions", "Feature scaling"], "language": "python", "name": "Standardize data", "path": "transformers/transformer_actions/column/standardize.py"}, "transformers/transformer_actions/column/impute.py:transformer:python:Fill in missing values:Column actions:Data cleaning": {"block_type": "transformer", "groups": ["Column actions", "Data cleaning"], "language": "python", "name": "Fill in missing values", "path": "transformers/transformer_actions/column/impute.py"}, "transformers/transformer_actions/column/remove_outliers.py:transformer:python:Remove outliers:Column actions:Data cleaning": {"block_type": "transformer", "groups": ["Column actions", "Data cleaning"], "language": "python", "name": "Remove outliers", "path": "transformers/transformer_actions/column/remove_outliers.py"}, "transformers/transformer_actions/column/diff.py:transformer:python:Calculate difference between values:Column actions:Feature extraction": {"block_type": "transformer", "groups": ["Column actions", "Feature extraction"], "language": "python", "name": "Calculate difference between values", "path": "transformers/transformer_actions/column/diff.py"}, "data_exporters/default.jinja:data_exporter:python:Base template (generic)": {"block_type": "data_exporter", "language": "python", "name": "Base template (generic)", "path": "data_exporters/default.jinja"}, "data_exporters/file.py:data_exporter:python:Local file": {"block_type": "data_exporter", "language": "python", "name": "Local file", "path": "data_exporters/file.py"}, "data_exporters/google_sheets.py:data_exporter:python:Google Sheets": {"block_type": "data_exporter", "language": "python", "name": "Google Sheets", "path": "data_exporters/google_sheets.py"}, "data_exporters/s3.py:data_exporter:python:Amazon S3:Data lakes": {"block_type": "data_exporter", "groups": ["Data lakes"], "language": "python", "name": "Amazon S3", "path": "data_exporters/s3.py"}, "data_exporters/azure_blob_storage.py:data_exporter:python:Azure Blob Storage:Data lakes": {"block_type": "data_exporter", "groups": ["Data lakes"], "language": "python", "name": "Azure Blob Storage", "path": "data_exporters/azure_blob_storage.py"}, "data_exporters/google_cloud_storage.py:data_exporter:python:Google Cloud Storage:Data lakes": {"block_type": "data_exporter", "groups": ["Data lakes"], "language": "python", "name": "Google Cloud Storage", "path": "data_exporters/google_cloud_storage.py"}, "data_exporters/redshift.py:data_exporter:python:Amazon Redshift:Data warehouses": {"block_type": "data_exporter", "groups": ["Data warehouses"], "language": "python", "name": "Amazon Redshift", "path": "data_exporters/redshift.py"}, "data_exporters/bigquery.py:data_exporter:python:Google BigQuery:Data warehouses": {"block_type": "data_exporter", "groups": ["Data warehouses"], "language": "python", "name": "Google BigQuery", "path": "data_exporters/bigquery.py"}, "data_exporters/snowflake.py:data_exporter:python:Snowflake:Data warehouses": {"block_type": "data_exporter", "groups": ["Data warehouses"], "language": "python", "name": "Snowflake", "path": "data_exporters/snowflake.py"}, "data_exporters/algolia.py:data_exporter:python:Algolia:Databases": {"block_type": "data_exporter", "groups": ["Databases"], "language": "python", "name": "Algolia", "path": "data_exporters/algolia.py"}, "data_exporters/chroma.py:data_exporter:python:Chroma:Databases": {"block_type": "data_exporter", "groups": ["Databases"], "language": "python", "name": "Chroma", "path": "data_exporters/chroma.py"}, "data_exporters/duckdb.py:data_exporter:python:DuckDB:Databases": {"block_type": "data_exporter", "groups": ["Databases"], "language": "python", "name": "DuckDB", "path": "data_exporters/duckdb.py"}, "data_exporters/mysql.py:data_exporter:python:MySQL:Databases": {"block_type": "data_exporter", "groups": ["Databases"], "language": "python", "name": "MySQL", "path": "data_exporters/mysql.py"}, "data_exporters/oracledb.py:data_exporter:python:OracleDB:Databases": {"block_type": "data_exporter", "groups": ["Databases"], "language": "python", "name": "OracleDB", "path": "data_exporters/oracledb.py"}, "data_exporters/postgres.py:data_exporter:python:PostgreSQL:Databases": {"block_type": "data_exporter", "groups": ["Databases"], "language": "python", "name": "PostgreSQL", "path": "data_exporters/postgres.py"}, "data_exporters/qdrant.py:data_exporter:python:Qdrant:Databases": {"block_type": "data_exporter", "groups": ["Databases"], "language": "python", "name": "Qdrant", "path": "data_exporters/qdrant.py"}, "data_exporters/weaviate.py:data_exporter:python:Weaviate:Databases": {"block_type": "data_exporter", "groups": ["Databases"], "language": "python", "name": "Weaviate", "path": "data_exporters/weaviate.py"}, "sensors/default.py:sensor:python:Base template (generic)": {"block_type": "sensor", "language": "python", "name": "Base template (generic)", "path": "sensors/default.py"}, "sensors/s3.py:sensor:python:Amazon S3:Data lakes": {"block_type": "sensor", "groups": ["Data lakes"], "language": "python", "name": "Amazon S3", "path": "sensors/s3.py"}, "sensors/google_cloud_storage.py:sensor:python:Google Cloud Storage:Data lakes": {"block_type": "sensor", "groups": ["Data lakes"], "language": "python", "name": "Google Cloud Storage", "path": "sensors/google_cloud_storage.py"}, "sensors/redshift.py:sensor:python:Amazon Redshift:Data warehouses": {"block_type": "sensor", "groups": ["Data warehouses"], "language": "python", "name": "Amazon Redshift", "path": "sensors/redshift.py"}, "sensors/bigquery.py:sensor:python:Google BigQuery:Data warehouses": {"block_type": "sensor", "groups": ["Data warehouses"], "language": "python", "name": "Google BigQuery", "path": "sensors/bigquery.py"}, "sensors/snowflake.py:sensor:python:Snowflake:Data warehouses": {"block_type": "sensor", "groups": ["Data warehouses"], "language": "python", "name": "Snowflake", "path": "sensors/snowflake.py"}, "sensors/mysql.py:sensor:python:MySQL:Databases": {"block_type": "sensor", "groups": ["Databases"], "language": "python", "name": "MySQL", "path": "sensors/mysql.py"}, "sensors/postgres.py:sensor:python:PostgreSQL:Databases": {"block_type": "sensor", "groups": ["Databases"], "language": "python", "name": "PostgreSQL", "path": "sensors/postgres.py"}}}